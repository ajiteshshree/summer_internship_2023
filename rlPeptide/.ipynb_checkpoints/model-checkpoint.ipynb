{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bae3cee6-3320-4342-b3bb-6008e9c956f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.0.1-cp39-cp39-win_amd64.whl (172.4 MB)\n",
      "Requirement already satisfied: filelock in d:\\anaconda_new\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: sympy in d:\\anaconda_new\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda_new\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: networkx in d:\\anaconda_new\\lib\\site-packages (from torch) (2.7.1)\n",
      "Requirement already satisfied: typing-extensions in d:\\anaconda_new\\lib\\site-packages (from torch) (4.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\anaconda_new\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\anaconda_new\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95817fb3-e08b-48e5-93de-bc10570c27d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import gym\n",
    "# from gym import spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import one_hot\n",
    "# from rdkit import Chem\n",
    "# from rdkit.Chem import Crippen\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbde1796-999b-4734-a360-56668c9c36c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 4\n",
    "standard_amino_acids = ['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
    "action_space = np.arange(0, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "285505e8-1d14-41fb-b12d-375c211a98ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_space):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.fc2 = nn.Linear(128, len(action_space))\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        action_probs = torch.softmax(self.fc2(x), dim=0)\n",
    "        return action_probs\n",
    "    #needs layers to improve peptide assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a11c3a34-d19b-46ca-8dd2-8fe281cc3eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyNetwork(\n",
       "  (fc1): Linear(in_features=4, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net = PolicyNetwork(state_size, action_space)\n",
    "policy_net #gives 20 probabilities for the 20 standard amino acids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcdb9b9b-2d65-4488-a7e1-21cacd3386f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = [-1,-1,-1,0]\n",
    "# se = []\n",
    "# for i in range(3):\n",
    "#     se.append(s)\n",
    "#     for j in range(len(s)):\n",
    "#         if s[j] == -1:\n",
    "#             s[j] = 9\n",
    "#             break\n",
    "#     s[3] = s[3] + 1\n",
    "    \n",
    "# print(se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4c9387f-bf50-4853-91c1-793d02f3a9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_episodes = 500\n",
    "len_peptide = 3\n",
    "max_steps = len_peptide\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b5d38f7-1f5a-4c52-843f-58e9c256566c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your reward function\n",
    "def compute_discounted_rewards(rewards, gamma):\n",
    "    discounted_rewards = [rewards[-1]]\n",
    "    for i in range(len(rewards) - 2, -1, -1):\n",
    "        discounted_rewards.insert(0, rewards[i] + gamma * discounted_rewards[0])\n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dd433bb-1a4d-4939-84a6-8f173df9bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your training loop\n",
    "def train():\n",
    "    # Set up your optimizer\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr= learning_rate) \n",
    "\n",
    "    # Training loop\n",
    "    for episode in range(num_episodes):\n",
    "        # Initialize the state\n",
    "        state = [-1,-1,-1,0] \n",
    "\n",
    "        # Lists to store the trajectory\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "\n",
    "        # Collect trajectory by interacting with the environment\n",
    "        for step in range(max_steps):\n",
    "        # Convert the state to a PyTorch tensor\n",
    "            \n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "            # Forward pass to get action probabilities\n",
    "            \n",
    "            action_probs = policy_net(state_tensor)\n",
    "\n",
    "            # Sample an action from the action probabilities\n",
    "            action = torch.multinomial(action_probs, 1).item()\n",
    "\n",
    "            # Execute the action and observe the next state and reward \n",
    "            next_state = action\n",
    "            reward = action_probs[action].item()\n",
    "                    \n",
    "            # Store the trajectory\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            # Update the state\n",
    "            for i in range(len(state)):\n",
    "                if state[i] == -1:\n",
    "                    state[i] = next_state\n",
    "                    break\n",
    "            state[3] = state[3] + 1\n",
    "\n",
    "        # Compute discounted rewards\n",
    "        discounted_rewards = compute_discounted_rewards(rewards, gamma)\n",
    "\n",
    "        # Convert trajectory to tensors\n",
    "        state_tensor = torch.tensor(states, dtype=torch.float32)\n",
    "        action_tensor = torch.tensor(actions, dtype=torch.int64)\n",
    "        reward_tensor = torch.tensor(discounted_rewards, dtype=torch.float32)\n",
    "\n",
    "        # Compute the loss\n",
    "        action_probs = policy_net(state_tensor)\n",
    "        selected_action_probs = action_probs.gather(1, action_tensor.unsqueeze(1)).squeeze()\n",
    "        loss = -torch.mean(torch.log(selected_action_probs) * reward_tensor)\n",
    "\n",
    "        # Update the policy network\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print training progress\n",
    "        print(f\"Episode: {episode}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "064776a5-0d4f-47e2-99ee-03c92bfee2fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Loss: 0.2939876317977905\n",
      "Episode: 1, Loss: 0.14314456284046173\n",
      "Episode: 2, Loss: 0.11114650964736938\n",
      "Episode: 3, Loss: 0.23640680313110352\n",
      "Episode: 4, Loss: 0.8206824660301208\n",
      "Episode: 5, Loss: 0.1563703715801239\n",
      "Episode: 6, Loss: 0.8006360530853271\n",
      "Episode: 7, Loss: 0.18392328917980194\n",
      "Episode: 8, Loss: 0.5483875870704651\n",
      "Episode: 9, Loss: 0.8860394358634949\n",
      "Episode: 10, Loss: 0.08212656527757645\n",
      "Episode: 11, Loss: 0.14249618351459503\n",
      "Episode: 12, Loss: 0.06705235689878464\n",
      "Episode: 13, Loss: 0.0626700222492218\n",
      "Episode: 14, Loss: 0.24359799921512604\n",
      "Episode: 15, Loss: 0.09353602677583694\n",
      "Episode: 16, Loss: 0.6402202248573303\n",
      "Episode: 17, Loss: 0.12179654836654663\n",
      "Episode: 18, Loss: 0.12630318105220795\n",
      "Episode: 19, Loss: 0.4670253098011017\n",
      "Episode: 20, Loss: 0.8006551861763\n",
      "Episode: 21, Loss: 0.17258095741271973\n",
      "Episode: 22, Loss: 0.10087627172470093\n",
      "Episode: 23, Loss: 0.6380236744880676\n",
      "Episode: 24, Loss: 0.4799175262451172\n",
      "Episode: 25, Loss: 0.341683030128479\n",
      "Episode: 26, Loss: 0.1956355720758438\n",
      "Episode: 27, Loss: 0.5264430046081543\n",
      "Episode: 28, Loss: 0.10140553116798401\n",
      "Episode: 29, Loss: 0.18172520399093628\n",
      "Episode: 30, Loss: 0.16806508600711823\n",
      "Episode: 31, Loss: 0.27341678738594055\n",
      "Episode: 32, Loss: 0.6440305709838867\n",
      "Episode: 33, Loss: 0.24875938892364502\n",
      "Episode: 34, Loss: 0.19035963714122772\n",
      "Episode: 35, Loss: 0.4418511688709259\n",
      "Episode: 36, Loss: 0.05925196036696434\n",
      "Episode: 37, Loss: 0.15610288083553314\n",
      "Episode: 38, Loss: 0.07676530629396439\n",
      "Episode: 39, Loss: 0.19310809671878815\n",
      "Episode: 40, Loss: 0.19161111116409302\n",
      "Episode: 41, Loss: 0.15795640647411346\n",
      "Episode: 42, Loss: 0.1572326123714447\n",
      "Episode: 43, Loss: 0.1019604504108429\n",
      "Episode: 44, Loss: 0.1549958437681198\n",
      "Episode: 45, Loss: 0.1132625862956047\n",
      "Episode: 46, Loss: 0.41452541947364807\n",
      "Episode: 47, Loss: 0.26752376556396484\n",
      "Episode: 48, Loss: 0.3386612832546234\n",
      "Episode: 49, Loss: 0.2251405566930771\n",
      "Episode: 50, Loss: 0.15690793097019196\n",
      "Episode: 51, Loss: 0.21164917945861816\n",
      "Episode: 52, Loss: 0.24400447309017181\n",
      "Episode: 53, Loss: 0.09772861748933792\n",
      "Episode: 54, Loss: 0.17928725481033325\n",
      "Episode: 55, Loss: 0.13442359864711761\n",
      "Episode: 56, Loss: 0.250307559967041\n",
      "Episode: 57, Loss: 0.09002676606178284\n",
      "Episode: 58, Loss: 0.12731747329235077\n",
      "Episode: 59, Loss: 0.12233567237854004\n",
      "Episode: 60, Loss: 0.16033117473125458\n",
      "Episode: 61, Loss: 0.17372286319732666\n",
      "Episode: 62, Loss: 0.2265966683626175\n",
      "Episode: 63, Loss: 0.15796035528182983\n",
      "Episode: 64, Loss: 0.26133736968040466\n",
      "Episode: 65, Loss: 0.27324631810188293\n",
      "Episode: 66, Loss: 0.12662693858146667\n",
      "Episode: 67, Loss: 0.14488449692726135\n",
      "Episode: 68, Loss: 0.1438472718000412\n",
      "Episode: 69, Loss: 0.39577797055244446\n",
      "Episode: 70, Loss: 0.4196830093860626\n",
      "Episode: 71, Loss: 0.276538610458374\n",
      "Episode: 72, Loss: 0.20842891931533813\n",
      "Episode: 73, Loss: 0.16784703731536865\n",
      "Episode: 74, Loss: 0.13717742264270782\n",
      "Episode: 75, Loss: 0.1781281977891922\n",
      "Episode: 76, Loss: 0.1857503056526184\n",
      "Episode: 77, Loss: 0.07655548304319382\n",
      "Episode: 78, Loss: 0.2930634021759033\n",
      "Episode: 79, Loss: 0.2594452202320099\n",
      "Episode: 80, Loss: 0.19885629415512085\n",
      "Episode: 81, Loss: 0.21791128814220428\n",
      "Episode: 82, Loss: 0.112888403236866\n",
      "Episode: 83, Loss: 0.1465245634317398\n",
      "Episode: 84, Loss: 0.2834007740020752\n",
      "Episode: 85, Loss: 0.426666259765625\n",
      "Episode: 86, Loss: 0.1725233644247055\n",
      "Episode: 87, Loss: 0.21810497343540192\n",
      "Episode: 88, Loss: 0.08972202986478806\n",
      "Episode: 89, Loss: 0.1749483197927475\n",
      "Episode: 90, Loss: 0.14067324995994568\n",
      "Episode: 91, Loss: 0.2203325480222702\n",
      "Episode: 92, Loss: 0.12238329648971558\n",
      "Episode: 93, Loss: 0.21488040685653687\n",
      "Episode: 94, Loss: 0.3121269941329956\n",
      "Episode: 95, Loss: 0.29781511425971985\n",
      "Episode: 96, Loss: 0.1365811824798584\n",
      "Episode: 97, Loss: 0.1745169758796692\n",
      "Episode: 98, Loss: 0.12381137162446976\n",
      "Episode: 99, Loss: 0.23377908766269684\n",
      "Episode: 100, Loss: 0.14205537736415863\n",
      "Episode: 101, Loss: 0.20373286306858063\n",
      "Episode: 102, Loss: 0.17269541323184967\n",
      "Episode: 103, Loss: 0.17974001169204712\n",
      "Episode: 104, Loss: 0.11826560646295547\n",
      "Episode: 105, Loss: 0.3361499309539795\n",
      "Episode: 106, Loss: 0.26875796914100647\n",
      "Episode: 107, Loss: 0.19201771914958954\n",
      "Episode: 108, Loss: 0.19668571650981903\n",
      "Episode: 109, Loss: 0.21983595192432404\n",
      "Episode: 110, Loss: 0.0599556528031826\n",
      "Episode: 111, Loss: 0.10101065784692764\n",
      "Episode: 112, Loss: 0.15355511009693146\n",
      "Episode: 113, Loss: 0.08496702462434769\n",
      "Episode: 114, Loss: 0.11446883529424667\n",
      "Episode: 115, Loss: 0.13898634910583496\n",
      "Episode: 116, Loss: 0.1792437583208084\n",
      "Episode: 117, Loss: 0.253391832113266\n",
      "Episode: 118, Loss: 0.20602546632289886\n",
      "Episode: 119, Loss: 0.262130469083786\n",
      "Episode: 120, Loss: 0.31093674898147583\n",
      "Episode: 121, Loss: 0.15045137703418732\n",
      "Episode: 122, Loss: 0.10968527942895889\n",
      "Episode: 123, Loss: 0.13924215734004974\n",
      "Episode: 124, Loss: 0.15519540011882782\n",
      "Episode: 125, Loss: 0.11235799640417099\n",
      "Episode: 126, Loss: 0.16327780485153198\n",
      "Episode: 127, Loss: 0.13218490779399872\n",
      "Episode: 128, Loss: 0.21147316694259644\n",
      "Episode: 129, Loss: 0.0680595114827156\n",
      "Episode: 130, Loss: 0.17434914410114288\n",
      "Episode: 131, Loss: 0.23948979377746582\n",
      "Episode: 132, Loss: 0.1974610835313797\n",
      "Episode: 133, Loss: 0.23796415328979492\n",
      "Episode: 134, Loss: 0.20024101436138153\n",
      "Episode: 135, Loss: 0.08528810739517212\n",
      "Episode: 136, Loss: 0.11948800832033157\n",
      "Episode: 137, Loss: 0.12025239318609238\n",
      "Episode: 138, Loss: 0.1451192945241928\n",
      "Episode: 139, Loss: 0.2072925716638565\n",
      "Episode: 140, Loss: 0.22352440655231476\n",
      "Episode: 141, Loss: 0.08067761361598969\n",
      "Episode: 142, Loss: 0.2382296770811081\n",
      "Episode: 143, Loss: 0.1443766951560974\n",
      "Episode: 144, Loss: 0.2184634804725647\n",
      "Episode: 145, Loss: 0.1554657369852066\n",
      "Episode: 146, Loss: 0.15041182935237885\n",
      "Episode: 147, Loss: 0.21985800564289093\n",
      "Episode: 148, Loss: 0.15344031155109406\n",
      "Episode: 149, Loss: 0.1103847548365593\n",
      "Episode: 150, Loss: 0.2589920163154602\n",
      "Episode: 151, Loss: 0.1759369820356369\n",
      "Episode: 152, Loss: 0.09512898325920105\n",
      "Episode: 153, Loss: 0.2803718149662018\n",
      "Episode: 154, Loss: 0.15623654425144196\n",
      "Episode: 155, Loss: 0.13432562351226807\n",
      "Episode: 156, Loss: 0.05528748407959938\n",
      "Episode: 157, Loss: 0.14201167225837708\n",
      "Episode: 158, Loss: 0.2046925276517868\n",
      "Episode: 159, Loss: 0.1303432732820511\n",
      "Episode: 160, Loss: 0.20841316878795624\n",
      "Episode: 161, Loss: 0.20987190306186676\n",
      "Episode: 162, Loss: 0.19580326974391937\n",
      "Episode: 163, Loss: 0.08487991243600845\n",
      "Episode: 164, Loss: 0.12309515476226807\n",
      "Episode: 165, Loss: 0.1378377079963684\n",
      "Episode: 166, Loss: 0.17211638391017914\n",
      "Episode: 167, Loss: 0.16402068734169006\n",
      "Episode: 168, Loss: 0.150456041097641\n",
      "Episode: 169, Loss: 0.10272619128227234\n",
      "Episode: 170, Loss: 0.14688673615455627\n",
      "Episode: 171, Loss: 0.1216675341129303\n",
      "Episode: 172, Loss: 0.1687604933977127\n",
      "Episode: 173, Loss: 0.15309083461761475\n",
      "Episode: 174, Loss: 0.0753459632396698\n",
      "Episode: 175, Loss: 0.23466672003269196\n",
      "Episode: 176, Loss: 0.14033281803131104\n",
      "Episode: 177, Loss: 0.15755783021450043\n",
      "Episode: 178, Loss: 0.08684050291776657\n",
      "Episode: 179, Loss: 0.09848644584417343\n",
      "Episode: 180, Loss: 0.18411533534526825\n",
      "Episode: 181, Loss: 0.18707293272018433\n",
      "Episode: 182, Loss: 0.17495010793209076\n",
      "Episode: 183, Loss: 0.12924496829509735\n",
      "Episode: 184, Loss: 0.24469085037708282\n",
      "Episode: 185, Loss: 0.24448390305042267\n",
      "Episode: 186, Loss: 0.07369091361761093\n",
      "Episode: 187, Loss: 0.2220984697341919\n",
      "Episode: 188, Loss: 0.12301803380250931\n",
      "Episode: 189, Loss: 0.295475572347641\n",
      "Episode: 190, Loss: 0.18575792014598846\n",
      "Episode: 191, Loss: 0.21894831955432892\n",
      "Episode: 192, Loss: 0.19010324776172638\n",
      "Episode: 193, Loss: 0.10441350191831589\n",
      "Episode: 194, Loss: 0.13815882802009583\n",
      "Episode: 195, Loss: 0.1528974026441574\n",
      "Episode: 196, Loss: 0.0736917182803154\n",
      "Episode: 197, Loss: 0.12229424715042114\n",
      "Episode: 198, Loss: 0.1199895441532135\n",
      "Episode: 199, Loss: 0.1400326043367386\n",
      "Episode: 200, Loss: 0.16445587575435638\n",
      "Episode: 201, Loss: 0.2588270604610443\n",
      "Episode: 202, Loss: 0.15142636001110077\n",
      "Episode: 203, Loss: 0.1922430545091629\n",
      "Episode: 204, Loss: 0.12318191677331924\n",
      "Episode: 205, Loss: 0.253368616104126\n",
      "Episode: 206, Loss: 0.1691296249628067\n",
      "Episode: 207, Loss: 0.1664080023765564\n",
      "Episode: 208, Loss: 0.16442148387432098\n",
      "Episode: 209, Loss: 0.16068251430988312\n",
      "Episode: 210, Loss: 0.16006134450435638\n",
      "Episode: 211, Loss: 0.1546955704689026\n",
      "Episode: 212, Loss: 0.1582275629043579\n",
      "Episode: 213, Loss: 0.04592715576291084\n",
      "Episode: 214, Loss: 0.1864403486251831\n",
      "Episode: 215, Loss: 0.22554892301559448\n",
      "Episode: 216, Loss: 0.09999408572912216\n",
      "Episode: 217, Loss: 0.13242869079113007\n",
      "Episode: 218, Loss: 0.14274436235427856\n",
      "Episode: 219, Loss: 0.1779245138168335\n",
      "Episode: 220, Loss: 0.27707529067993164\n",
      "Episode: 221, Loss: 0.16664105653762817\n",
      "Episode: 222, Loss: 0.22838492691516876\n",
      "Episode: 223, Loss: 0.21015553176403046\n",
      "Episode: 224, Loss: 0.13231141865253448\n",
      "Episode: 225, Loss: 0.12479797005653381\n",
      "Episode: 226, Loss: 0.22366876900196075\n",
      "Episode: 227, Loss: 0.15902237594127655\n",
      "Episode: 228, Loss: 0.1351175457239151\n",
      "Episode: 229, Loss: 0.16293425858020782\n",
      "Episode: 230, Loss: 0.3065577447414398\n",
      "Episode: 231, Loss: 0.11821109056472778\n",
      "Episode: 232, Loss: 0.16157586872577667\n",
      "Episode: 233, Loss: 0.12183689326047897\n",
      "Episode: 234, Loss: 0.10684952139854431\n",
      "Episode: 235, Loss: 0.12134471535682678\n",
      "Episode: 236, Loss: 0.19608376920223236\n",
      "Episode: 237, Loss: 0.16466635465621948\n",
      "Episode: 238, Loss: 0.0953509509563446\n",
      "Episode: 239, Loss: 0.09059145301580429\n",
      "Episode: 240, Loss: 0.19949893653392792\n",
      "Episode: 241, Loss: 0.1133909597992897\n",
      "Episode: 242, Loss: 0.3135410249233246\n",
      "Episode: 243, Loss: 0.12175286561250687\n",
      "Episode: 244, Loss: 0.16100086271762848\n",
      "Episode: 245, Loss: 0.13069787621498108\n",
      "Episode: 246, Loss: 0.17956407368183136\n",
      "Episode: 247, Loss: 0.2468070238828659\n",
      "Episode: 248, Loss: 0.24722005426883698\n",
      "Episode: 249, Loss: 0.14846976101398468\n",
      "Episode: 250, Loss: 0.09586214274168015\n",
      "Episode: 251, Loss: 0.10816607624292374\n",
      "Episode: 252, Loss: 0.06327612698078156\n",
      "Episode: 253, Loss: 0.13548032939434052\n",
      "Episode: 254, Loss: 0.1799212098121643\n",
      "Episode: 255, Loss: 0.2145659178495407\n",
      "Episode: 256, Loss: 0.14460577070713043\n",
      "Episode: 257, Loss: 0.16037936508655548\n",
      "Episode: 258, Loss: 0.2585698664188385\n",
      "Episode: 259, Loss: 0.06565950810909271\n",
      "Episode: 260, Loss: 0.15512242913246155\n",
      "Episode: 261, Loss: 0.16036485135555267\n",
      "Episode: 262, Loss: 0.1340898871421814\n",
      "Episode: 263, Loss: 0.12612959742546082\n",
      "Episode: 264, Loss: 0.19291579723358154\n",
      "Episode: 265, Loss: 0.11827459186315536\n",
      "Episode: 266, Loss: 0.10344546288251877\n",
      "Episode: 267, Loss: 0.20675480365753174\n",
      "Episode: 268, Loss: 0.34441879391670227\n",
      "Episode: 269, Loss: 0.11024586111307144\n",
      "Episode: 270, Loss: 0.16971206665039062\n",
      "Episode: 271, Loss: 0.10080906003713608\n",
      "Episode: 272, Loss: 0.15604975819587708\n",
      "Episode: 273, Loss: 0.22198987007141113\n",
      "Episode: 274, Loss: 0.13153024017810822\n",
      "Episode: 275, Loss: 0.14932484924793243\n",
      "Episode: 276, Loss: 0.13891233503818512\n",
      "Episode: 277, Loss: 0.07534999400377274\n",
      "Episode: 278, Loss: 0.19474749267101288\n",
      "Episode: 279, Loss: 0.15668044984340668\n",
      "Episode: 280, Loss: 0.15874123573303223\n",
      "Episode: 281, Loss: 0.10797041654586792\n",
      "Episode: 282, Loss: 0.16709910333156586\n",
      "Episode: 283, Loss: 0.17739206552505493\n",
      "Episode: 284, Loss: 0.27010151743888855\n",
      "Episode: 285, Loss: 0.19492827355861664\n",
      "Episode: 286, Loss: 0.10934821516275406\n",
      "Episode: 287, Loss: 0.1588306725025177\n",
      "Episode: 288, Loss: 0.18832294642925262\n",
      "Episode: 289, Loss: 0.12095394730567932\n",
      "Episode: 290, Loss: 0.16230802237987518\n",
      "Episode: 291, Loss: 0.14011633396148682\n",
      "Episode: 292, Loss: 0.218051478266716\n",
      "Episode: 293, Loss: 0.14392536878585815\n",
      "Episode: 294, Loss: 0.09854161739349365\n",
      "Episode: 295, Loss: 0.3376527726650238\n",
      "Episode: 296, Loss: 0.20539212226867676\n",
      "Episode: 297, Loss: 0.16234157979488373\n",
      "Episode: 298, Loss: 0.11329684406518936\n",
      "Episode: 299, Loss: 0.142516627907753\n",
      "Episode: 300, Loss: 0.1437206119298935\n",
      "Episode: 301, Loss: 0.10290006548166275\n",
      "Episode: 302, Loss: 0.21156436204910278\n",
      "Episode: 303, Loss: 0.085828997194767\n",
      "Episode: 304, Loss: 0.16990385949611664\n",
      "Episode: 305, Loss: 0.1564045399427414\n",
      "Episode: 306, Loss: 0.14758454263210297\n",
      "Episode: 307, Loss: 0.1368974894285202\n",
      "Episode: 308, Loss: 0.07398663461208344\n",
      "Episode: 309, Loss: 0.13881459832191467\n",
      "Episode: 310, Loss: 0.08665630966424942\n",
      "Episode: 311, Loss: 0.16734425723552704\n",
      "Episode: 312, Loss: 0.1049220860004425\n",
      "Episode: 313, Loss: 0.08581515401601791\n",
      "Episode: 314, Loss: 0.15549223124980927\n",
      "Episode: 315, Loss: 0.10033013671636581\n",
      "Episode: 316, Loss: 0.13422907888889313\n",
      "Episode: 317, Loss: 0.04983013868331909\n",
      "Episode: 318, Loss: 0.45909860730171204\n",
      "Episode: 319, Loss: 0.18240422010421753\n",
      "Episode: 320, Loss: 0.08303076773881912\n",
      "Episode: 321, Loss: 0.18325822055339813\n",
      "Episode: 322, Loss: 0.1372508704662323\n",
      "Episode: 323, Loss: 0.12702392041683197\n",
      "Episode: 324, Loss: 0.165533185005188\n",
      "Episode: 325, Loss: 0.12682388722896576\n",
      "Episode: 326, Loss: 0.11697853356599808\n",
      "Episode: 327, Loss: 0.15209625661373138\n",
      "Episode: 328, Loss: 0.1676589995622635\n",
      "Episode: 329, Loss: 0.25884971022605896\n",
      "Episode: 330, Loss: 0.11149545758962631\n",
      "Episode: 331, Loss: 0.13967682421207428\n",
      "Episode: 332, Loss: 0.13138218224048615\n",
      "Episode: 333, Loss: 0.1424764096736908\n",
      "Episode: 334, Loss: 0.13096994161605835\n",
      "Episode: 335, Loss: 0.2109415978193283\n",
      "Episode: 336, Loss: 0.15750350058078766\n",
      "Episode: 337, Loss: 0.14583700895309448\n",
      "Episode: 338, Loss: 0.17139393091201782\n",
      "Episode: 339, Loss: 0.13184456527233124\n",
      "Episode: 340, Loss: 0.17720907926559448\n",
      "Episode: 341, Loss: 0.13280290365219116\n",
      "Episode: 342, Loss: 0.1381307989358902\n",
      "Episode: 343, Loss: 0.14373013377189636\n",
      "Episode: 344, Loss: 0.23242390155792236\n",
      "Episode: 345, Loss: 0.15289762616157532\n",
      "Episode: 346, Loss: 0.24450421333312988\n",
      "Episode: 347, Loss: 0.11584939807653427\n",
      "Episode: 348, Loss: 0.13643169403076172\n",
      "Episode: 349, Loss: 0.10014643520116806\n",
      "Episode: 350, Loss: 0.21573452651500702\n",
      "Episode: 351, Loss: 0.1728469729423523\n",
      "Episode: 352, Loss: 0.10591816902160645\n",
      "Episode: 353, Loss: 0.12396708130836487\n",
      "Episode: 354, Loss: 0.07756906747817993\n",
      "Episode: 355, Loss: 0.11667240411043167\n",
      "Episode: 356, Loss: 0.15680360794067383\n",
      "Episode: 357, Loss: 0.22928349673748016\n",
      "Episode: 358, Loss: 0.20462489128112793\n",
      "Episode: 359, Loss: 0.09994421154260635\n",
      "Episode: 360, Loss: 0.20890440046787262\n",
      "Episode: 361, Loss: 0.17443795502185822\n",
      "Episode: 362, Loss: 0.1850985437631607\n",
      "Episode: 363, Loss: 0.2317676991224289\n",
      "Episode: 364, Loss: 0.08133001625537872\n",
      "Episode: 365, Loss: 0.08267268538475037\n",
      "Episode: 366, Loss: 0.14388780295848846\n",
      "Episode: 367, Loss: 0.17033620178699493\n",
      "Episode: 368, Loss: 0.3595626652240753\n",
      "Episode: 369, Loss: 0.05748729407787323\n",
      "Episode: 370, Loss: 0.1432127058506012\n",
      "Episode: 371, Loss: 0.14466634392738342\n",
      "Episode: 372, Loss: 0.16256499290466309\n",
      "Episode: 373, Loss: 0.23255926370620728\n",
      "Episode: 374, Loss: 0.1335974782705307\n",
      "Episode: 375, Loss: 0.19771814346313477\n",
      "Episode: 376, Loss: 0.19053643941879272\n",
      "Episode: 377, Loss: 0.11314915865659714\n",
      "Episode: 378, Loss: 0.14912624657154083\n",
      "Episode: 379, Loss: 0.17722868919372559\n",
      "Episode: 380, Loss: 0.0853179320693016\n",
      "Episode: 381, Loss: 0.16772527992725372\n",
      "Episode: 382, Loss: 0.11422140151262283\n",
      "Episode: 383, Loss: 0.15616551041603088\n",
      "Episode: 384, Loss: 0.09416967630386353\n",
      "Episode: 385, Loss: 0.252559095621109\n",
      "Episode: 386, Loss: 0.24805229902267456\n",
      "Episode: 387, Loss: 0.22329990565776825\n",
      "Episode: 388, Loss: 0.25021591782569885\n",
      "Episode: 389, Loss: 0.34564340114593506\n",
      "Episode: 390, Loss: 0.13925710320472717\n",
      "Episode: 391, Loss: 0.150775745511055\n",
      "Episode: 392, Loss: 0.1728861927986145\n",
      "Episode: 393, Loss: 0.1425124555826187\n",
      "Episode: 394, Loss: 0.10992160439491272\n",
      "Episode: 395, Loss: 0.11427123099565506\n",
      "Episode: 396, Loss: 0.09668461233377457\n",
      "Episode: 397, Loss: 0.22425000369548798\n",
      "Episode: 398, Loss: 0.253025621175766\n",
      "Episode: 399, Loss: 0.12541091442108154\n",
      "Episode: 400, Loss: 0.07926330715417862\n",
      "Episode: 401, Loss: 0.1258736550807953\n",
      "Episode: 402, Loss: 0.2036311775445938\n",
      "Episode: 403, Loss: 0.10270882397890091\n",
      "Episode: 404, Loss: 0.12283843010663986\n",
      "Episode: 405, Loss: 0.1947055608034134\n",
      "Episode: 406, Loss: 0.19269709289073944\n",
      "Episode: 407, Loss: 0.15794262290000916\n",
      "Episode: 408, Loss: 0.19011497497558594\n",
      "Episode: 409, Loss: 0.22347599267959595\n",
      "Episode: 410, Loss: 0.12925191223621368\n",
      "Episode: 411, Loss: 0.07827671617269516\n",
      "Episode: 412, Loss: 0.21763575077056885\n",
      "Episode: 413, Loss: 0.09677883982658386\n",
      "Episode: 414, Loss: 0.1026201918721199\n",
      "Episode: 415, Loss: 0.11155153065919876\n",
      "Episode: 416, Loss: 0.0881749764084816\n",
      "Episode: 417, Loss: 0.15924866497516632\n",
      "Episode: 418, Loss: 0.15165157616138458\n",
      "Episode: 419, Loss: 0.22568543255329132\n",
      "Episode: 420, Loss: 0.11481832712888718\n",
      "Episode: 421, Loss: 0.2642044126987457\n",
      "Episode: 422, Loss: 0.1112666055560112\n",
      "Episode: 423, Loss: 0.27464166283607483\n",
      "Episode: 424, Loss: 0.22740565240383148\n",
      "Episode: 425, Loss: 0.1491016298532486\n",
      "Episode: 426, Loss: 0.06447848677635193\n",
      "Episode: 427, Loss: 0.11446536332368851\n",
      "Episode: 428, Loss: 0.16838477551937103\n",
      "Episode: 429, Loss: 0.07462051510810852\n",
      "Episode: 430, Loss: 0.04829515889286995\n",
      "Episode: 431, Loss: 0.08374156802892685\n",
      "Episode: 432, Loss: 0.1900571584701538\n",
      "Episode: 433, Loss: 0.23255787789821625\n",
      "Episode: 434, Loss: 0.21704941987991333\n",
      "Episode: 435, Loss: 0.06204541027545929\n",
      "Episode: 436, Loss: 0.14159049093723297\n",
      "Episode: 437, Loss: 0.2086818963289261\n",
      "Episode: 438, Loss: 0.09538698941469193\n",
      "Episode: 439, Loss: 0.22711117565631866\n",
      "Episode: 440, Loss: 0.19409428536891937\n",
      "Episode: 441, Loss: 0.0797024518251419\n",
      "Episode: 442, Loss: 0.17438991367816925\n",
      "Episode: 443, Loss: 0.10178572684526443\n",
      "Episode: 444, Loss: 0.22823242843151093\n",
      "Episode: 445, Loss: 0.17528848350048065\n",
      "Episode: 446, Loss: 0.44024530053138733\n",
      "Episode: 447, Loss: 0.3530399799346924\n",
      "Episode: 448, Loss: 0.23739643394947052\n",
      "Episode: 449, Loss: 0.1689673215150833\n",
      "Episode: 450, Loss: 0.1832796335220337\n",
      "Episode: 451, Loss: 0.19476540386676788\n",
      "Episode: 452, Loss: 0.08830576390028\n",
      "Episode: 453, Loss: 0.11236713081598282\n",
      "Episode: 454, Loss: 0.17195703089237213\n",
      "Episode: 455, Loss: 0.20750178396701813\n",
      "Episode: 456, Loss: 0.09977849572896957\n",
      "Episode: 457, Loss: 0.16690798103809357\n",
      "Episode: 458, Loss: 0.1957879662513733\n",
      "Episode: 459, Loss: 0.100233294069767\n",
      "Episode: 460, Loss: 0.15624912083148956\n",
      "Episode: 461, Loss: 0.2448711395263672\n",
      "Episode: 462, Loss: 0.12606681883335114\n",
      "Episode: 463, Loss: 0.2399841547012329\n",
      "Episode: 464, Loss: 0.18919728696346283\n",
      "Episode: 465, Loss: 0.25089889764785767\n",
      "Episode: 466, Loss: 0.17885762453079224\n",
      "Episode: 467, Loss: 0.1594739705324173\n",
      "Episode: 468, Loss: 0.2041207104921341\n",
      "Episode: 469, Loss: 0.16002844274044037\n",
      "Episode: 470, Loss: 0.17099164426326752\n",
      "Episode: 471, Loss: 0.17356795072555542\n",
      "Episode: 472, Loss: 0.15365880727767944\n",
      "Episode: 473, Loss: 0.112676240503788\n",
      "Episode: 474, Loss: 0.3022509813308716\n",
      "Episode: 475, Loss: 0.2274238020181656\n",
      "Episode: 476, Loss: 0.24642930924892426\n",
      "Episode: 477, Loss: 0.15763920545578003\n",
      "Episode: 478, Loss: 0.2692202627658844\n",
      "Episode: 479, Loss: 0.08961000293493271\n",
      "Episode: 480, Loss: 0.12865419685840607\n",
      "Episode: 481, Loss: 0.09957551211118698\n",
      "Episode: 482, Loss: 0.20001570880413055\n",
      "Episode: 483, Loss: 0.1898125559091568\n",
      "Episode: 484, Loss: 0.11855628341436386\n",
      "Episode: 485, Loss: 0.17602181434631348\n",
      "Episode: 486, Loss: 0.16278484463691711\n",
      "Episode: 487, Loss: 0.12078475952148438\n",
      "Episode: 488, Loss: 0.1355387568473816\n",
      "Episode: 489, Loss: 0.15687023103237152\n",
      "Episode: 490, Loss: 0.150445818901062\n",
      "Episode: 491, Loss: 0.08111348748207092\n",
      "Episode: 492, Loss: 0.16425934433937073\n",
      "Episode: 493, Loss: 0.14697043597698212\n",
      "Episode: 494, Loss: 0.1996450573205948\n",
      "Episode: 495, Loss: 0.15912146866321564\n",
      "Episode: 496, Loss: 0.11569835990667343\n",
      "Episode: 497, Loss: 0.16774220764636993\n",
      "Episode: 498, Loss: 0.19479085505008698\n",
      "Episode: 499, Loss: 0.16220800578594208\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e663957b-7ef4-447e-a69c-978cd9b4e2e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
