{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bae3cee6-3320-4342-b3bb-6008e9c956f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\anaconda_new\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: sympy in d:\\anaconda_new\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: fsspec in d:\\anaconda_new\\lib\\site-packages (from torch) (2022.2.0)\n",
      "Requirement already satisfied: filelock in d:\\anaconda_new\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: networkx in d:\\anaconda_new\\lib\\site-packages (from torch) (2.7.1)\n",
      "Requirement already satisfied: typing-extensions in d:\\anaconda_new\\lib\\site-packages (from torch) (4.1.1)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda_new\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\anaconda_new\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\anaconda_new\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c089c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in d:\\anaconda_new\\lib\\site-packages (1.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in d:\\anaconda_new\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.4 in d:\\anaconda_new\\lib\\site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in d:\\anaconda_new\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda_new\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95817fb3-e08b-48e5-93de-bc10570c27d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import gym\n",
    "# from gym import spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import one_hot\n",
    "# from rdkit import Chem\n",
    "# from rdkit.Chem import Crippen\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbde1796-999b-4734-a360-56668c9c36c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 4\n",
    "standard_amino_acids = ['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
    "action_space = np.arange(0, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "285505e8-1d14-41fb-b12d-375c211a98ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_space):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.fc2 = nn.Linear(128, len(action_space))\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        action_probs = torch.softmax(self.fc2(x), dim=0)\n",
    "        return action_probs\n",
    "    #needs layers to improve peptide assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a11c3a34-d19b-46ca-8dd2-8fe281cc3eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyNetwork(\n",
       "  (fc1): Linear(in_features=4, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net = PolicyNetwork(state_size, action_space)\n",
    "policy_net #gives 20 probabilities for the 20 standard amino acids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcdb9b9b-2d65-4488-a7e1-21cacd3386f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = [-1,-1,-1,0]\n",
    "# se = []\n",
    "# for i in range(3):\n",
    "#     se.append(s)\n",
    "#     for j in range(len(s)):\n",
    "#         if s[j] == -1:\n",
    "#             s[j] = 9\n",
    "#             break\n",
    "#     s[3] = s[3] + 1\n",
    "    \n",
    "# print(se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4c9387f-bf50-4853-91c1-793d02f3a9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_episodes = 500\n",
    "len_peptide = 3\n",
    "max_steps = len_peptide\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b5d38f7-1f5a-4c52-843f-58e9c256566c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your reward function\n",
    "def compute_discounted_rewards(rewards, gamma):\n",
    "    discounted_rewards = [rewards[-1]]\n",
    "    for i in range(len(rewards) - 2, -1, -1):\n",
    "        discounted_rewards.insert(0, rewards[i] + gamma * discounted_rewards[0])\n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dd433bb-1a4d-4939-84a6-8f173df9bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your training loop\n",
    "def train():\n",
    "    # Set up your optimizer\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr= learning_rate) \n",
    "\n",
    "    # Training loop\n",
    "    for episode in range(num_episodes):\n",
    "        # Initialize the state\n",
    "        state = [-1,-1,-1,0] \n",
    "\n",
    "        # Lists to store the trajectory\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "\n",
    "        # Collect trajectory by interacting with the environment\n",
    "        for step in range(max_steps):\n",
    "        # Convert the state to a PyTorch tensor\n",
    "            \n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "            # Forward pass to get action probabilities\n",
    "            \n",
    "            action_probs = policy_net(state_tensor)\n",
    "\n",
    "            # Sample an action from the action probabilities\n",
    "            action = torch.multinomial(action_probs, 1).item()\n",
    "\n",
    "            # Execute the action and observe the next state and reward \n",
    "            next_state = action\n",
    "            reward = action_probs[action].item()\n",
    "                    \n",
    "            # Store the trajectory\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            # Update the state\n",
    "            for i in range(len(state)):\n",
    "                if state[i] == -1:\n",
    "                    state[i] = next_state\n",
    "                    break\n",
    "            state[3] = state[3] + 1\n",
    "\n",
    "        # Compute discounted rewards\n",
    "        discounted_rewards = compute_discounted_rewards(rewards, gamma)\n",
    "\n",
    "        # Convert trajectory to tensors\n",
    "        state_tensor = torch.tensor(states, dtype=torch.float32)\n",
    "        action_tensor = torch.tensor(actions, dtype=torch.int64)\n",
    "        reward_tensor = torch.tensor(discounted_rewards, dtype=torch.float32)\n",
    "\n",
    "        # Compute the loss\n",
    "        action_probs = policy_net(state_tensor)\n",
    "        selected_action_probs = action_probs.gather(1, action_tensor.unsqueeze(1)).squeeze()\n",
    "        loss = -torch.mean(torch.log(selected_action_probs) * reward_tensor)\n",
    "\n",
    "        # Update the policy network\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print training progress\n",
    "        print(f\"Episode: {episode}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "064776a5-0d4f-47e2-99ee-03c92bfee2fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Loss: 0.5430271625518799\n",
      "Episode: 1, Loss: 0.9528985023498535\n",
      "Episode: 2, Loss: 0.9086907505989075\n",
      "Episode: 3, Loss: 0.18107479810714722\n",
      "Episode: 4, Loss: 0.6095492839813232\n",
      "Episode: 5, Loss: 0.11210090667009354\n",
      "Episode: 6, Loss: 0.4600731134414673\n",
      "Episode: 7, Loss: 0.2432212233543396\n",
      "Episode: 8, Loss: 0.10943662375211716\n",
      "Episode: 9, Loss: 0.8772689700126648\n",
      "Episode: 10, Loss: 0.7673117518424988\n",
      "Episode: 11, Loss: 0.18759524822235107\n",
      "Episode: 12, Loss: 0.6430943608283997\n",
      "Episode: 13, Loss: 0.13941650092601776\n",
      "Episode: 14, Loss: 0.8355398178100586\n",
      "Episode: 15, Loss: 0.1703762412071228\n",
      "Episode: 16, Loss: 0.11849549412727356\n",
      "Episode: 17, Loss: 0.2464107722043991\n",
      "Episode: 18, Loss: 0.11423813551664352\n",
      "Episode: 19, Loss: 0.3126469552516937\n",
      "Episode: 20, Loss: 0.39979493618011475\n",
      "Episode: 21, Loss: 0.248043492436409\n",
      "Episode: 22, Loss: 0.07659133523702621\n",
      "Episode: 23, Loss: 0.8324288725852966\n",
      "Episode: 24, Loss: 0.149540513753891\n",
      "Episode: 25, Loss: 0.26571550965309143\n",
      "Episode: 26, Loss: 0.17192351818084717\n",
      "Episode: 27, Loss: 0.18675415217876434\n",
      "Episode: 28, Loss: 0.22729958593845367\n",
      "Episode: 29, Loss: 0.28967007994651794\n",
      "Episode: 30, Loss: 0.1120380163192749\n",
      "Episode: 31, Loss: 0.1753951907157898\n",
      "Episode: 32, Loss: 0.16980010271072388\n",
      "Episode: 33, Loss: 0.08595401048660278\n",
      "Episode: 34, Loss: 0.32802432775497437\n",
      "Episode: 35, Loss: 0.13188576698303223\n",
      "Episode: 36, Loss: 0.08480995148420334\n",
      "Episode: 37, Loss: 0.16732029616832733\n",
      "Episode: 38, Loss: 0.3059721291065216\n",
      "Episode: 39, Loss: 0.1668701171875\n",
      "Episode: 40, Loss: 0.14770729839801788\n",
      "Episode: 41, Loss: 0.24523766338825226\n",
      "Episode: 42, Loss: 0.17380231618881226\n",
      "Episode: 43, Loss: 0.12573903799057007\n",
      "Episode: 44, Loss: 0.522325336933136\n",
      "Episode: 45, Loss: 0.48787784576416016\n",
      "Episode: 46, Loss: 0.1713017374277115\n",
      "Episode: 47, Loss: 0.2932063043117523\n",
      "Episode: 48, Loss: 0.5285344123840332\n",
      "Episode: 49, Loss: 0.3980729579925537\n",
      "Episode: 50, Loss: 0.12734362483024597\n",
      "Episode: 51, Loss: 0.17525887489318848\n",
      "Episode: 52, Loss: 0.20444773137569427\n",
      "Episode: 53, Loss: 0.3766383230686188\n",
      "Episode: 54, Loss: 0.4067467153072357\n",
      "Episode: 55, Loss: 0.20059578120708466\n",
      "Episode: 56, Loss: 0.11958277225494385\n",
      "Episode: 57, Loss: 0.07578391581773758\n",
      "Episode: 58, Loss: 0.17316024005413055\n",
      "Episode: 59, Loss: 0.12649807333946228\n",
      "Episode: 60, Loss: 0.13777326047420502\n",
      "Episode: 61, Loss: 0.17202681303024292\n",
      "Episode: 62, Loss: 0.13232766091823578\n",
      "Episode: 63, Loss: 0.13900823891162872\n",
      "Episode: 64, Loss: 0.2316911816596985\n",
      "Episode: 65, Loss: 0.25459805130958557\n",
      "Episode: 66, Loss: 0.2315972000360489\n",
      "Episode: 67, Loss: 0.13387350738048553\n",
      "Episode: 68, Loss: 0.5211924910545349\n",
      "Episode: 69, Loss: 0.522868812084198\n",
      "Episode: 70, Loss: 0.131602942943573\n",
      "Episode: 71, Loss: 0.18066562712192535\n",
      "Episode: 72, Loss: 0.14231431484222412\n",
      "Episode: 73, Loss: 0.12587116658687592\n",
      "Episode: 74, Loss: 0.15547619760036469\n",
      "Episode: 75, Loss: 0.5372539162635803\n",
      "Episode: 76, Loss: 0.23241902887821198\n",
      "Episode: 77, Loss: 0.06736872345209122\n",
      "Episode: 78, Loss: 0.4680028259754181\n",
      "Episode: 79, Loss: 0.16916780173778534\n",
      "Episode: 80, Loss: 0.1289566457271576\n",
      "Episode: 81, Loss: 0.4713585078716278\n",
      "Episode: 82, Loss: 0.38413119316101074\n",
      "Episode: 83, Loss: 0.13965439796447754\n",
      "Episode: 84, Loss: 0.20027868449687958\n",
      "Episode: 85, Loss: 0.4020240008831024\n",
      "Episode: 86, Loss: 0.12959714233875275\n",
      "Episode: 87, Loss: 0.4696485996246338\n",
      "Episode: 88, Loss: 0.4172884523868561\n",
      "Episode: 89, Loss: 0.17888861894607544\n",
      "Episode: 90, Loss: 0.1597568392753601\n",
      "Episode: 91, Loss: 0.13337664306163788\n",
      "Episode: 92, Loss: 0.33107516169548035\n",
      "Episode: 93, Loss: 0.16423021256923676\n",
      "Episode: 94, Loss: 0.19110800325870514\n",
      "Episode: 95, Loss: 0.13468648493289948\n",
      "Episode: 96, Loss: 0.13587325811386108\n",
      "Episode: 97, Loss: 0.41632404923439026\n",
      "Episode: 98, Loss: 0.1851397603750229\n",
      "Episode: 99, Loss: 0.17314769327640533\n",
      "Episode: 100, Loss: 0.17749112844467163\n",
      "Episode: 101, Loss: 0.4260355532169342\n",
      "Episode: 102, Loss: 0.09311901777982712\n",
      "Episode: 103, Loss: 0.1378798633813858\n",
      "Episode: 104, Loss: 0.3130228519439697\n",
      "Episode: 105, Loss: 0.10178849846124649\n",
      "Episode: 106, Loss: 0.20691682398319244\n",
      "Episode: 107, Loss: 0.2361418604850769\n",
      "Episode: 108, Loss: 0.23044629395008087\n",
      "Episode: 109, Loss: 0.40276315808296204\n",
      "Episode: 110, Loss: 0.08889218419790268\n",
      "Episode: 111, Loss: 0.1066841185092926\n",
      "Episode: 112, Loss: 0.15674971044063568\n",
      "Episode: 113, Loss: 0.10725245624780655\n",
      "Episode: 114, Loss: 0.12986956536769867\n",
      "Episode: 115, Loss: 0.07083951681852341\n",
      "Episode: 116, Loss: 0.24643822014331818\n",
      "Episode: 117, Loss: 0.19523954391479492\n",
      "Episode: 118, Loss: 0.18308961391448975\n",
      "Episode: 119, Loss: 0.11189208179712296\n",
      "Episode: 120, Loss: 0.13660097122192383\n",
      "Episode: 121, Loss: 0.11666969209909439\n",
      "Episode: 122, Loss: 0.2327667474746704\n",
      "Episode: 123, Loss: 0.15526451170444489\n",
      "Episode: 124, Loss: 0.17213450372219086\n",
      "Episode: 125, Loss: 0.34040120244026184\n",
      "Episode: 126, Loss: 0.20319350063800812\n",
      "Episode: 127, Loss: 0.12547670304775238\n",
      "Episode: 128, Loss: 0.0915381982922554\n",
      "Episode: 129, Loss: 0.13390742242336273\n",
      "Episode: 130, Loss: 0.22712957859039307\n",
      "Episode: 131, Loss: 0.3416779339313507\n",
      "Episode: 132, Loss: 0.15637123584747314\n",
      "Episode: 133, Loss: 0.12108556181192398\n",
      "Episode: 134, Loss: 0.14377619326114655\n",
      "Episode: 135, Loss: 0.27007755637168884\n",
      "Episode: 136, Loss: 0.16174161434173584\n",
      "Episode: 137, Loss: 0.19130642712116241\n",
      "Episode: 138, Loss: 0.1325194388628006\n",
      "Episode: 139, Loss: 0.39271774888038635\n",
      "Episode: 140, Loss: 0.1560426950454712\n",
      "Episode: 141, Loss: 0.09386386722326279\n",
      "Episode: 142, Loss: 0.3172273337841034\n",
      "Episode: 143, Loss: 0.10157016664743423\n",
      "Episode: 144, Loss: 0.2788383364677429\n",
      "Episode: 145, Loss: 0.2745775282382965\n",
      "Episode: 146, Loss: 0.13743382692337036\n",
      "Episode: 147, Loss: 0.1058744266629219\n",
      "Episode: 148, Loss: 0.386081725358963\n",
      "Episode: 149, Loss: 0.38097962737083435\n",
      "Episode: 150, Loss: 0.131185382604599\n",
      "Episode: 151, Loss: 0.1194673702120781\n",
      "Episode: 152, Loss: 0.11786166578531265\n",
      "Episode: 153, Loss: 0.14593689143657684\n",
      "Episode: 154, Loss: 0.2040995955467224\n",
      "Episode: 155, Loss: 0.36111390590667725\n",
      "Episode: 156, Loss: 0.16465312242507935\n",
      "Episode: 157, Loss: 0.16647547483444214\n",
      "Episode: 158, Loss: 0.1644294708967209\n",
      "Episode: 159, Loss: 0.275324285030365\n",
      "Episode: 160, Loss: 0.11189985275268555\n",
      "Episode: 161, Loss: 0.2945522964000702\n",
      "Episode: 162, Loss: 0.13746891915798187\n",
      "Episode: 163, Loss: 0.1767921894788742\n",
      "Episode: 164, Loss: 0.1260332465171814\n",
      "Episode: 165, Loss: 0.11065632849931717\n",
      "Episode: 166, Loss: 0.3141944706439972\n",
      "Episode: 167, Loss: 0.18584370613098145\n",
      "Episode: 168, Loss: 0.17742092907428741\n",
      "Episode: 169, Loss: 0.16192275285720825\n",
      "Episode: 170, Loss: 0.22217099368572235\n",
      "Episode: 171, Loss: 0.1975991129875183\n",
      "Episode: 172, Loss: 0.1698002964258194\n",
      "Episode: 173, Loss: 0.0636783167719841\n",
      "Episode: 174, Loss: 0.11066211014986038\n",
      "Episode: 175, Loss: 0.45947957038879395\n",
      "Episode: 176, Loss: 0.46298637986183167\n",
      "Episode: 177, Loss: 0.11014141887426376\n",
      "Episode: 178, Loss: 0.134013369679451\n",
      "Episode: 179, Loss: 0.10986442118883133\n",
      "Episode: 180, Loss: 0.1564844846725464\n",
      "Episode: 181, Loss: 0.2357606142759323\n",
      "Episode: 182, Loss: 0.13882984220981598\n",
      "Episode: 183, Loss: 0.09515552967786789\n",
      "Episode: 184, Loss: 0.12412643432617188\n",
      "Episode: 185, Loss: 0.25137293338775635\n",
      "Episode: 186, Loss: 0.4051227569580078\n",
      "Episode: 187, Loss: 0.11052942276000977\n",
      "Episode: 188, Loss: 0.19701890647411346\n",
      "Episode: 189, Loss: 0.14301158487796783\n",
      "Episode: 190, Loss: 0.20387613773345947\n",
      "Episode: 191, Loss: 0.05535192787647247\n",
      "Episode: 192, Loss: 0.12887714803218842\n",
      "Episode: 193, Loss: 0.14743947982788086\n",
      "Episode: 194, Loss: 0.15320082008838654\n",
      "Episode: 195, Loss: 0.12372211366891861\n",
      "Episode: 196, Loss: 0.14959903061389923\n",
      "Episode: 197, Loss: 0.21205466985702515\n",
      "Episode: 198, Loss: 0.1387036144733429\n",
      "Episode: 199, Loss: 0.1225275993347168\n",
      "Episode: 200, Loss: 0.10441982001066208\n",
      "Episode: 201, Loss: 0.11310616135597229\n",
      "Episode: 202, Loss: 0.33139726519584656\n",
      "Episode: 203, Loss: 0.08932793885469437\n",
      "Episode: 204, Loss: 0.15599489212036133\n",
      "Episode: 205, Loss: 0.10381688922643661\n",
      "Episode: 206, Loss: 0.3526429235935211\n",
      "Episode: 207, Loss: 0.11171086877584457\n",
      "Episode: 208, Loss: 0.19260619580745697\n",
      "Episode: 209, Loss: 0.13602176308631897\n",
      "Episode: 210, Loss: 0.1278189867734909\n",
      "Episode: 211, Loss: 0.22740168869495392\n",
      "Episode: 212, Loss: 0.10578043013811111\n",
      "Episode: 213, Loss: 0.1692059487104416\n",
      "Episode: 214, Loss: 0.15147064626216888\n",
      "Episode: 215, Loss: 0.21561329066753387\n",
      "Episode: 216, Loss: 0.10601741820573807\n",
      "Episode: 217, Loss: 0.3769854009151459\n",
      "Episode: 218, Loss: 0.4282956123352051\n",
      "Episode: 219, Loss: 0.11652976274490356\n",
      "Episode: 220, Loss: 0.14865083992481232\n",
      "Episode: 221, Loss: 0.7337629199028015\n",
      "Episode: 222, Loss: 0.5129022598266602\n",
      "Episode: 223, Loss: 0.22802917659282684\n",
      "Episode: 224, Loss: 0.1384129375219345\n",
      "Episode: 225, Loss: 0.6179261803627014\n",
      "Episode: 226, Loss: 0.5312338471412659\n",
      "Episode: 227, Loss: 0.18004049360752106\n",
      "Episode: 228, Loss: 0.7725541591644287\n",
      "Episode: 229, Loss: 0.14206193387508392\n",
      "Episode: 230, Loss: 0.6514660716056824\n",
      "Episode: 231, Loss: 0.4918915033340454\n",
      "Episode: 232, Loss: 0.18898124992847443\n",
      "Episode: 233, Loss: 0.33886316418647766\n",
      "Episode: 234, Loss: 0.23090369999408722\n",
      "Episode: 235, Loss: 0.13780641555786133\n",
      "Episode: 236, Loss: 0.126504048705101\n",
      "Episode: 237, Loss: 0.11623233556747437\n",
      "Episode: 238, Loss: 0.203576922416687\n",
      "Episode: 239, Loss: 0.27145716547966003\n",
      "Episode: 240, Loss: 0.20962673425674438\n",
      "Episode: 241, Loss: 0.1828618198633194\n",
      "Episode: 242, Loss: 0.19964911043643951\n",
      "Episode: 243, Loss: 0.14379212260246277\n",
      "Episode: 244, Loss: 0.09615493565797806\n",
      "Episode: 245, Loss: 0.14555291831493378\n",
      "Episode: 246, Loss: 0.14329089224338531\n",
      "Episode: 247, Loss: 0.17473912239074707\n",
      "Episode: 248, Loss: 0.16528595983982086\n",
      "Episode: 249, Loss: 0.06269701570272446\n",
      "Episode: 250, Loss: 0.17822504043579102\n",
      "Episode: 251, Loss: 0.15239910781383514\n",
      "Episode: 252, Loss: 0.17689771950244904\n",
      "Episode: 253, Loss: 0.16682159900665283\n",
      "Episode: 254, Loss: 0.1965271383523941\n",
      "Episode: 255, Loss: 0.11565923690795898\n",
      "Episode: 256, Loss: 0.18547923862934113\n",
      "Episode: 257, Loss: 0.18900422751903534\n",
      "Episode: 258, Loss: 0.14166167378425598\n",
      "Episode: 259, Loss: 0.07463758438825607\n",
      "Episode: 260, Loss: 0.11554133892059326\n",
      "Episode: 261, Loss: 0.7957605719566345\n",
      "Episode: 262, Loss: 0.7636350989341736\n",
      "Episode: 263, Loss: 0.12968622148036957\n",
      "Episode: 264, Loss: 0.12078355997800827\n",
      "Episode: 265, Loss: 0.39241671562194824\n",
      "Episode: 266, Loss: 0.13783879578113556\n",
      "Episode: 267, Loss: 0.25663575530052185\n",
      "Episode: 268, Loss: 0.09861865639686584\n",
      "Episode: 269, Loss: 0.10916227102279663\n",
      "Episode: 270, Loss: 0.41864344477653503\n",
      "Episode: 271, Loss: 0.15101127326488495\n",
      "Episode: 272, Loss: 0.30551159381866455\n",
      "Episode: 273, Loss: 0.12383187562227249\n",
      "Episode: 274, Loss: 0.09454966336488724\n",
      "Episode: 275, Loss: 0.15882350504398346\n",
      "Episode: 276, Loss: 0.5418534874916077\n",
      "Episode: 277, Loss: 0.11196553707122803\n",
      "Episode: 278, Loss: 0.1001366451382637\n",
      "Episode: 279, Loss: 0.16462953388690948\n",
      "Episode: 280, Loss: 0.2338770180940628\n",
      "Episode: 281, Loss: 0.1874259114265442\n",
      "Episode: 282, Loss: 0.11934121698141098\n",
      "Episode: 283, Loss: 0.14431093633174896\n",
      "Episode: 284, Loss: 0.1157991886138916\n",
      "Episode: 285, Loss: 0.1449453979730606\n",
      "Episode: 286, Loss: 0.11476242542266846\n",
      "Episode: 287, Loss: 0.0824190303683281\n",
      "Episode: 288, Loss: 0.32725420594215393\n",
      "Episode: 289, Loss: 0.19495151937007904\n",
      "Episode: 290, Loss: 0.13516925275325775\n",
      "Episode: 291, Loss: 0.31732475757598877\n",
      "Episode: 292, Loss: 0.1399836391210556\n",
      "Episode: 293, Loss: 0.41160860657691956\n",
      "Episode: 294, Loss: 0.14066334068775177\n",
      "Episode: 295, Loss: 0.14652134478092194\n",
      "Episode: 296, Loss: 0.1796015352010727\n",
      "Episode: 297, Loss: 0.3397541046142578\n",
      "Episode: 298, Loss: 0.17940402030944824\n",
      "Episode: 299, Loss: 0.1797722429037094\n",
      "Episode: 300, Loss: 0.09519663453102112\n",
      "Episode: 301, Loss: 0.3667587339878082\n",
      "Episode: 302, Loss: 0.14605580270290375\n",
      "Episode: 303, Loss: 0.12277373671531677\n",
      "Episode: 304, Loss: 0.35574719309806824\n",
      "Episode: 305, Loss: 0.3626162111759186\n",
      "Episode: 306, Loss: 0.15027301013469696\n",
      "Episode: 307, Loss: 0.17821021378040314\n",
      "Episode: 308, Loss: 0.1508917361497879\n",
      "Episode: 309, Loss: 0.14846882224082947\n",
      "Episode: 310, Loss: 0.24357099831104279\n",
      "Episode: 311, Loss: 0.07475342601537704\n",
      "Episode: 312, Loss: 0.1243218183517456\n",
      "Episode: 313, Loss: 0.18652862310409546\n",
      "Episode: 314, Loss: 0.3150796890258789\n",
      "Episode: 315, Loss: 0.1272866576910019\n",
      "Episode: 316, Loss: 0.14559637010097504\n",
      "Episode: 317, Loss: 0.08392240852117538\n",
      "Episode: 318, Loss: 0.29609301686286926\n",
      "Episode: 319, Loss: 0.1690129041671753\n",
      "Episode: 320, Loss: 0.06180607154965401\n",
      "Episode: 321, Loss: 0.15516142547130585\n",
      "Episode: 322, Loss: 0.05340835452079773\n",
      "Episode: 323, Loss: 0.06645781546831131\n",
      "Episode: 324, Loss: 0.1620439738035202\n",
      "Episode: 325, Loss: 0.27283975481987\n",
      "Episode: 326, Loss: 0.17612195014953613\n",
      "Episode: 327, Loss: 0.11079278588294983\n",
      "Episode: 328, Loss: 0.1359107941389084\n",
      "Episode: 329, Loss: 0.1545245349407196\n",
      "Episode: 330, Loss: 0.12784437835216522\n",
      "Episode: 331, Loss: 0.2021959275007248\n",
      "Episode: 332, Loss: 0.07447139173746109\n",
      "Episode: 333, Loss: 0.13509084284305573\n",
      "Episode: 334, Loss: 0.12428011745214462\n",
      "Episode: 335, Loss: 0.12320622056722641\n",
      "Episode: 336, Loss: 0.06395269185304642\n",
      "Episode: 337, Loss: 0.14596810936927795\n",
      "Episode: 338, Loss: 0.1272839456796646\n",
      "Episode: 339, Loss: 0.16651129722595215\n",
      "Episode: 340, Loss: 0.17179419100284576\n",
      "Episode: 341, Loss: 0.2675432562828064\n",
      "Episode: 342, Loss: 0.1663445681333542\n",
      "Episode: 343, Loss: 0.5669602751731873\n",
      "Episode: 344, Loss: 0.5318300724029541\n",
      "Episode: 345, Loss: 0.16182370483875275\n",
      "Episode: 346, Loss: 0.17039917409420013\n",
      "Episode: 347, Loss: 0.12643149495124817\n",
      "Episode: 348, Loss: 0.1643683761358261\n",
      "Episode: 349, Loss: 0.1470210999250412\n",
      "Episode: 350, Loss: 0.17939235270023346\n",
      "Episode: 351, Loss: 0.14429117739200592\n",
      "Episode: 352, Loss: 0.1477028727531433\n",
      "Episode: 353, Loss: 0.15028385818004608\n",
      "Episode: 354, Loss: 0.1877211183309555\n",
      "Episode: 355, Loss: 0.2889065444469452\n",
      "Episode: 356, Loss: 0.12750153243541718\n",
      "Episode: 357, Loss: 0.19350172579288483\n",
      "Episode: 358, Loss: 0.2117847055196762\n",
      "Episode: 359, Loss: 0.18327827751636505\n",
      "Episode: 360, Loss: 0.15259522199630737\n",
      "Episode: 361, Loss: 0.16316188871860504\n",
      "Episode: 362, Loss: 0.20228911936283112\n",
      "Episode: 363, Loss: 0.10070826858282089\n",
      "Episode: 364, Loss: 0.12882447242736816\n",
      "Episode: 365, Loss: 0.19482333958148956\n",
      "Episode: 366, Loss: 0.14378924667835236\n",
      "Episode: 367, Loss: 0.14389668405056\n",
      "Episode: 368, Loss: 0.09754041582345963\n",
      "Episode: 369, Loss: 0.14391504228115082\n",
      "Episode: 370, Loss: 0.13370215892791748\n",
      "Episode: 371, Loss: 0.1252458691596985\n",
      "Episode: 372, Loss: 0.1140202209353447\n",
      "Episode: 373, Loss: 0.17563118040561676\n",
      "Episode: 374, Loss: 0.11606907099485397\n",
      "Episode: 375, Loss: 0.21497654914855957\n",
      "Episode: 376, Loss: 0.13757538795471191\n",
      "Episode: 377, Loss: 0.1625087857246399\n",
      "Episode: 378, Loss: 0.16995815932750702\n",
      "Episode: 379, Loss: 0.13441960513591766\n",
      "Episode: 380, Loss: 0.08676014095544815\n",
      "Episode: 381, Loss: 0.19367462396621704\n",
      "Episode: 382, Loss: 0.16405516862869263\n",
      "Episode: 383, Loss: 0.16098321974277496\n",
      "Episode: 384, Loss: 0.32337185740470886\n",
      "Episode: 385, Loss: 0.20349960029125214\n",
      "Episode: 386, Loss: 0.1482064127922058\n",
      "Episode: 387, Loss: 0.11040478944778442\n",
      "Episode: 388, Loss: 0.19310040771961212\n",
      "Episode: 389, Loss: 0.12771844863891602\n",
      "Episode: 390, Loss: 0.1605958342552185\n",
      "Episode: 391, Loss: 0.12707401812076569\n",
      "Episode: 392, Loss: 0.11585097759962082\n",
      "Episode: 393, Loss: 0.18329398334026337\n",
      "Episode: 394, Loss: 0.14691105484962463\n",
      "Episode: 395, Loss: 0.2152726799249649\n",
      "Episode: 396, Loss: 0.1954713612794876\n",
      "Episode: 397, Loss: 0.14543253183364868\n",
      "Episode: 398, Loss: 0.28911885619163513\n",
      "Episode: 399, Loss: 0.203815296292305\n",
      "Episode: 400, Loss: 0.15012194216251373\n",
      "Episode: 401, Loss: 0.13906343281269073\n",
      "Episode: 402, Loss: 0.13741105794906616\n",
      "Episode: 403, Loss: 0.2178269773721695\n",
      "Episode: 404, Loss: 0.11133857816457748\n",
      "Episode: 405, Loss: 0.12523576617240906\n",
      "Episode: 406, Loss: 0.10761413723230362\n",
      "Episode: 407, Loss: 0.32544979453086853\n",
      "Episode: 408, Loss: 0.11727429181337357\n",
      "Episode: 409, Loss: 0.15708979964256287\n",
      "Episode: 410, Loss: 0.11554959416389465\n",
      "Episode: 411, Loss: 0.12214553356170654\n",
      "Episode: 412, Loss: 0.15393440425395966\n",
      "Episode: 413, Loss: 0.10361508280038834\n",
      "Episode: 414, Loss: 0.1129949688911438\n",
      "Episode: 415, Loss: 0.16715474426746368\n",
      "Episode: 416, Loss: 0.19962196052074432\n",
      "Episode: 417, Loss: 0.07576332241296768\n",
      "Episode: 418, Loss: 0.08996647596359253\n",
      "Episode: 419, Loss: 0.16873054206371307\n",
      "Episode: 420, Loss: 0.11397805064916611\n",
      "Episode: 421, Loss: 0.17958611249923706\n",
      "Episode: 422, Loss: 0.1760084629058838\n",
      "Episode: 423, Loss: 0.0701347216963768\n",
      "Episode: 424, Loss: 0.18843679130077362\n",
      "Episode: 425, Loss: 0.08110445737838745\n",
      "Episode: 426, Loss: 0.3522448241710663\n",
      "Episode: 427, Loss: 0.18517029285430908\n",
      "Episode: 428, Loss: 0.11001843214035034\n",
      "Episode: 429, Loss: 0.2901143729686737\n",
      "Episode: 430, Loss: 0.09568921476602554\n",
      "Episode: 431, Loss: 0.2116093635559082\n",
      "Episode: 432, Loss: 0.21130304038524628\n",
      "Episode: 433, Loss: 0.11256486177444458\n",
      "Episode: 434, Loss: 0.15285104513168335\n",
      "Episode: 435, Loss: 0.2883872091770172\n",
      "Episode: 436, Loss: 0.1500299721956253\n",
      "Episode: 437, Loss: 0.1987331360578537\n",
      "Episode: 438, Loss: 0.16283421218395233\n",
      "Episode: 439, Loss: 0.14580799639225006\n",
      "Episode: 440, Loss: 0.13044120371341705\n",
      "Episode: 441, Loss: 0.17348986864089966\n",
      "Episode: 442, Loss: 0.13103102147579193\n",
      "Episode: 443, Loss: 0.264422208070755\n",
      "Episode: 444, Loss: 0.19463340938091278\n",
      "Episode: 445, Loss: 0.23922620713710785\n",
      "Episode: 446, Loss: 0.12606222927570343\n",
      "Episode: 447, Loss: 0.2971174418926239\n",
      "Episode: 448, Loss: 0.20103959739208221\n",
      "Episode: 449, Loss: 0.15665407478809357\n",
      "Episode: 450, Loss: 0.11677184700965881\n",
      "Episode: 451, Loss: 0.1599966436624527\n",
      "Episode: 452, Loss: 0.17132693529129028\n",
      "Episode: 453, Loss: 0.16106726229190826\n",
      "Episode: 454, Loss: 0.140660360455513\n",
      "Episode: 455, Loss: 0.24368131160736084\n",
      "Episode: 456, Loss: 0.22722482681274414\n",
      "Episode: 457, Loss: 0.15173715353012085\n",
      "Episode: 458, Loss: 0.14924456179141998\n",
      "Episode: 459, Loss: 0.18888284265995026\n",
      "Episode: 460, Loss: 0.11117378622293472\n",
      "Episode: 461, Loss: 0.1450580209493637\n",
      "Episode: 462, Loss: 0.12836101651191711\n",
      "Episode: 463, Loss: 0.36925938725471497\n",
      "Episode: 464, Loss: 0.11881998181343079\n",
      "Episode: 465, Loss: 0.16634579002857208\n",
      "Episode: 466, Loss: 0.15137070417404175\n",
      "Episode: 467, Loss: 0.15370279550552368\n",
      "Episode: 468, Loss: 0.35489174723625183\n",
      "Episode: 469, Loss: 0.1843665987253189\n",
      "Episode: 470, Loss: 0.1128033697605133\n",
      "Episode: 471, Loss: 0.10540229827165604\n",
      "Episode: 472, Loss: 0.13981355726718903\n",
      "Episode: 473, Loss: 0.23359453678131104\n",
      "Episode: 474, Loss: 0.09567286819219589\n",
      "Episode: 475, Loss: 0.16925249993801117\n",
      "Episode: 476, Loss: 0.14278121292591095\n",
      "Episode: 477, Loss: 0.2811853289604187\n",
      "Episode: 478, Loss: 0.13268308341503143\n",
      "Episode: 479, Loss: 0.32670143246650696\n",
      "Episode: 480, Loss: 0.190329909324646\n",
      "Episode: 481, Loss: 0.1409991830587387\n",
      "Episode: 482, Loss: 0.14173246920108795\n",
      "Episode: 483, Loss: 0.21437685191631317\n",
      "Episode: 484, Loss: 0.20822101831436157\n",
      "Episode: 485, Loss: 0.08872825652360916\n",
      "Episode: 486, Loss: 0.15450017154216766\n",
      "Episode: 487, Loss: 0.139138326048851\n",
      "Episode: 488, Loss: 0.18284936249256134\n",
      "Episode: 489, Loss: 0.2636544406414032\n",
      "Episode: 490, Loss: 0.16932368278503418\n",
      "Episode: 491, Loss: 0.1431940644979477\n",
      "Episode: 492, Loss: 0.12178745120763779\n",
      "Episode: 493, Loss: 0.30274489521980286\n",
      "Episode: 494, Loss: 0.11766011267900467\n",
      "Episode: 495, Loss: 0.15870466828346252\n",
      "Episode: 496, Loss: 0.1168256402015686\n",
      "Episode: 497, Loss: 0.2526126503944397\n",
      "Episode: 498, Loss: 0.14065410196781158\n",
      "Episode: 499, Loss: 0.08217089623212814\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e663957b-7ef4-447e-a69c-978cd9b4e2e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
