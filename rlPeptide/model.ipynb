{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95817fb3-e08b-48e5-93de-bc10570c27d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import one_hot\n",
    "# from rdkit import Chem\n",
    "# from rdkit.Chem import Crippen\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbde1796-999b-4734-a360-56668c9c36c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 4\n",
    "standard_amino_acids = ['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
    "action_space = np.arange(0, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "285505e8-1d14-41fb-b12d-375c211a98ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_space):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.fc2 = nn.Linear(128, len(action_space))\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        action_probs = torch.softmax(self.fc2(x), dim=0)\n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a11c3a34-d19b-46ca-8dd2-8fe281cc3eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyNetwork(\n",
       "  (fc1): Linear(in_features=4, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net = PolicyNetwork(state_size, action_space)\n",
    "policy_net #gives 20 probabilities for the 20 standard amino acids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcdb9b9b-2d65-4488-a7e1-21cacd3386f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = [-1,-1,-1,0]\n",
    "# se = []\n",
    "# for i in range(3):\n",
    "#     se.append(s)\n",
    "#     for j in range(len(s)):\n",
    "#         if s[j] == -1:\n",
    "#             s[j] = 9\n",
    "#             break\n",
    "#     s[3] = s[3] + 1\n",
    "    \n",
    "# print(se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4c9387f-bf50-4853-91c1-793d02f3a9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_episodes = 500\n",
    "len_peptide = 3\n",
    "max_steps = len_peptide\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b5d38f7-1f5a-4c52-843f-58e9c256566c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your reward function\n",
    "def compute_discounted_rewards(rewards, gamma):\n",
    "    discounted_rewards = [rewards[-1]]\n",
    "    for i in range(len(rewards) - 2, -1, -1):\n",
    "        discounted_rewards.insert(0, rewards[i] + gamma * discounted_rewards[0])\n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dd433bb-1a4d-4939-84a6-8f173df9bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your training loop\n",
    "def train():\n",
    "    # Set up your optimizer\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr= learning_rate) \n",
    "\n",
    "    # Training loop\n",
    "    for episode in range(num_episodes):\n",
    "        # Initialize the state\n",
    "        state = [-1,-1,-1,0] \n",
    "\n",
    "        # Lists to store the trajectory\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "\n",
    "        # Collect trajectory by interacting with the environment\n",
    "        for step in range(max_steps):\n",
    "        # Convert the state to a PyTorch tensor\n",
    "            \n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "            # Forward pass to get action probabilities\n",
    "            \n",
    "            action_probs = policy_net(state_tensor)\n",
    "\n",
    "            # Sample an action from the action probabilities\n",
    "            action = torch.multinomial(action_probs, 1).item()\n",
    "\n",
    "            # Execute the action and observe the next state and reward \n",
    "            next_state = action\n",
    "            reward = action_probs[action].item()\n",
    "                    \n",
    "            # Store the trajectory\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            # Update the state\n",
    "            for i in range(len(state)):\n",
    "                if state[i] == -1:\n",
    "                    state[i] = next_state\n",
    "                    break\n",
    "            state[3] = state[3] + 1\n",
    "\n",
    "        # Compute discounted rewards\n",
    "        discounted_rewards = compute_discounted_rewards(rewards, gamma)\n",
    "\n",
    "        # Convert trajectory to tensors\n",
    "        state_tensor = torch.tensor(states, dtype=torch.float32)\n",
    "        action_tensor = torch.tensor(actions, dtype=torch.int64)\n",
    "        reward_tensor = torch.tensor(discounted_rewards, dtype=torch.float32)\n",
    "\n",
    "        # Compute the loss\n",
    "        action_probs = policy_net(state_tensor)\n",
    "        selected_action_probs = action_probs.gather(1, action_tensor.unsqueeze(1)).squeeze()\n",
    "        loss = -torch.mean(torch.log(selected_action_probs) * reward_tensor)\n",
    "\n",
    "        # Update the policy network\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print training progress\n",
    "        print(f\"Episode: {episode}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "064776a5-0d4f-47e2-99ee-03c92bfee2fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Loss: 0.2527870833873749\n",
      "Episode: 1, Loss: 0.12812061607837677\n",
      "Episode: 2, Loss: 0.4495612382888794\n",
      "Episode: 3, Loss: 0.3946632444858551\n",
      "Episode: 4, Loss: 0.31685808300971985\n",
      "Episode: 5, Loss: 0.23426277935504913\n",
      "Episode: 6, Loss: 0.2937659025192261\n",
      "Episode: 7, Loss: 0.7505591511726379\n",
      "Episode: 8, Loss: 0.10025864839553833\n",
      "Episode: 9, Loss: 0.7924635410308838\n",
      "Episode: 10, Loss: 0.47618845105171204\n",
      "Episode: 11, Loss: 0.06063328683376312\n",
      "Episode: 12, Loss: 0.8527063727378845\n",
      "Episode: 13, Loss: 0.17512117326259613\n",
      "Episode: 14, Loss: 0.08644694834947586\n",
      "Episode: 15, Loss: 0.11897142976522446\n",
      "Episode: 16, Loss: 0.9271313548088074\n",
      "Episode: 17, Loss: 0.22364778816699982\n",
      "Episode: 18, Loss: 0.24418038129806519\n",
      "Episode: 19, Loss: 0.4341578781604767\n",
      "Episode: 20, Loss: 0.1770927459001541\n",
      "Episode: 21, Loss: 0.9450812935829163\n",
      "Episode: 22, Loss: 0.11655717343091965\n",
      "Episode: 23, Loss: 0.18812452256679535\n",
      "Episode: 24, Loss: 0.1277303397655487\n",
      "Episode: 25, Loss: 0.2960887849330902\n",
      "Episode: 26, Loss: 0.7513626217842102\n",
      "Episode: 27, Loss: 0.5198512077331543\n",
      "Episode: 28, Loss: 0.1313926875591278\n",
      "Episode: 29, Loss: 0.3191637098789215\n",
      "Episode: 30, Loss: 0.7662814259529114\n",
      "Episode: 31, Loss: 0.11655322462320328\n",
      "Episode: 32, Loss: 0.500226616859436\n",
      "Episode: 33, Loss: 0.9432802796363831\n",
      "Episode: 34, Loss: 0.21396489441394806\n",
      "Episode: 35, Loss: 0.10480033606290817\n",
      "Episode: 36, Loss: 0.19985412061214447\n",
      "Episode: 37, Loss: 0.7862668037414551\n",
      "Episode: 38, Loss: 0.12781238555908203\n",
      "Episode: 39, Loss: 0.07098013162612915\n",
      "Episode: 40, Loss: 0.13469016551971436\n",
      "Episode: 41, Loss: 0.12326383590698242\n",
      "Episode: 42, Loss: 0.673362672328949\n",
      "Episode: 43, Loss: 0.17665128409862518\n",
      "Episode: 44, Loss: 0.18781830370426178\n",
      "Episode: 45, Loss: 0.25214868783950806\n",
      "Episode: 46, Loss: 0.690960168838501\n",
      "Episode: 47, Loss: 0.16899317502975464\n",
      "Episode: 48, Loss: 0.7278409600257874\n",
      "Episode: 49, Loss: 0.1250690370798111\n",
      "Episode: 50, Loss: 0.11789420992136002\n",
      "Episode: 51, Loss: 0.30297836661338806\n",
      "Episode: 52, Loss: 0.7101006507873535\n",
      "Episode: 53, Loss: 0.5815702080726624\n",
      "Episode: 54, Loss: 0.14227916300296783\n",
      "Episode: 55, Loss: 0.8515588641166687\n",
      "Episode: 56, Loss: 0.1327255219221115\n",
      "Episode: 57, Loss: 0.45464691519737244\n",
      "Episode: 58, Loss: 0.6869966387748718\n",
      "Episode: 59, Loss: 0.8773314356803894\n",
      "Episode: 60, Loss: 0.6414554715156555\n",
      "Episode: 61, Loss: 0.2138463258743286\n",
      "Episode: 62, Loss: 0.17416620254516602\n",
      "Episode: 63, Loss: 0.21709305047988892\n",
      "Episode: 64, Loss: 0.23901039361953735\n",
      "Episode: 65, Loss: 0.6194598078727722\n",
      "Episode: 66, Loss: 0.24042348563671112\n",
      "Episode: 67, Loss: 0.33344754576683044\n",
      "Episode: 68, Loss: 0.8552532196044922\n",
      "Episode: 69, Loss: 0.1435704082250595\n",
      "Episode: 70, Loss: 0.18962959945201874\n",
      "Episode: 71, Loss: 0.13475823402404785\n",
      "Episode: 72, Loss: 0.2703479528427124\n",
      "Episode: 73, Loss: 0.38252750039100647\n",
      "Episode: 74, Loss: 0.15305417776107788\n",
      "Episode: 75, Loss: 0.16787183284759521\n",
      "Episode: 76, Loss: 0.23928330838680267\n",
      "Episode: 77, Loss: 0.44452038407325745\n",
      "Episode: 78, Loss: 0.8118175864219666\n",
      "Episode: 79, Loss: 0.09825123101472855\n",
      "Episode: 80, Loss: 0.38223159313201904\n",
      "Episode: 81, Loss: 0.22515548765659332\n",
      "Episode: 82, Loss: 0.14359654486179352\n",
      "Episode: 83, Loss: 0.6833229660987854\n",
      "Episode: 84, Loss: 0.21588461101055145\n",
      "Episode: 85, Loss: 0.13730719685554504\n",
      "Episode: 86, Loss: 0.07594598829746246\n",
      "Episode: 87, Loss: 0.13677579164505005\n",
      "Episode: 88, Loss: 0.08898415416479111\n",
      "Episode: 89, Loss: 0.17631159722805023\n",
      "Episode: 90, Loss: 0.5200005173683167\n",
      "Episode: 91, Loss: 0.10459505766630173\n",
      "Episode: 92, Loss: 0.0858134925365448\n",
      "Episode: 93, Loss: 0.10008957237005234\n",
      "Episode: 94, Loss: 0.5472124218940735\n",
      "Episode: 95, Loss: 0.6876053214073181\n",
      "Episode: 96, Loss: 0.4857535660266876\n",
      "Episode: 97, Loss: 0.10252808779478073\n",
      "Episode: 98, Loss: 0.09988268464803696\n",
      "Episode: 99, Loss: 0.09819047898054123\n",
      "Episode: 100, Loss: 0.36781153082847595\n",
      "Episode: 101, Loss: 0.22101306915283203\n",
      "Episode: 102, Loss: 0.11387272924184799\n",
      "Episode: 103, Loss: 0.10468008369207382\n",
      "Episode: 104, Loss: 0.6922985911369324\n",
      "Episode: 105, Loss: 0.36883726716041565\n",
      "Episode: 106, Loss: 0.2778542637825012\n",
      "Episode: 107, Loss: 0.1323864609003067\n",
      "Episode: 108, Loss: 0.16601037979125977\n",
      "Episode: 109, Loss: 0.46702226996421814\n",
      "Episode: 110, Loss: 0.1928216964006424\n",
      "Episode: 111, Loss: 0.5694337487220764\n",
      "Episode: 112, Loss: 0.3607650101184845\n",
      "Episode: 113, Loss: 0.050482045859098434\n",
      "Episode: 114, Loss: 0.19744426012039185\n",
      "Episode: 115, Loss: 0.2815721929073334\n",
      "Episode: 116, Loss: 0.3305719196796417\n",
      "Episode: 117, Loss: 0.7832391262054443\n",
      "Episode: 118, Loss: 0.3023693859577179\n",
      "Episode: 119, Loss: 0.12783120572566986\n",
      "Episode: 120, Loss: 0.3333599269390106\n",
      "Episode: 121, Loss: 0.715594470500946\n",
      "Episode: 122, Loss: 0.12316804379224777\n",
      "Episode: 123, Loss: 0.2876448333263397\n",
      "Episode: 124, Loss: 0.15426842868328094\n",
      "Episode: 125, Loss: 0.30729883909225464\n",
      "Episode: 126, Loss: 0.44478675723075867\n",
      "Episode: 127, Loss: 0.5228914618492126\n",
      "Episode: 128, Loss: 0.18445806205272675\n",
      "Episode: 129, Loss: 0.6730821132659912\n",
      "Episode: 130, Loss: 0.32679247856140137\n",
      "Episode: 131, Loss: 0.4007669985294342\n",
      "Episode: 132, Loss: 0.22602935135364532\n",
      "Episode: 133, Loss: 0.2458759993314743\n",
      "Episode: 134, Loss: 0.355122834444046\n",
      "Episode: 135, Loss: 0.7112412452697754\n",
      "Episode: 136, Loss: 0.07396143674850464\n",
      "Episode: 137, Loss: 0.46429458260536194\n",
      "Episode: 138, Loss: 0.26749834418296814\n",
      "Episode: 139, Loss: 0.7794559001922607\n",
      "Episode: 140, Loss: 0.6123886704444885\n",
      "Episode: 141, Loss: 0.1727687567472458\n",
      "Episode: 142, Loss: 0.35663607716560364\n",
      "Episode: 143, Loss: 0.47679653763771057\n",
      "Episode: 144, Loss: 0.48940137028694153\n",
      "Episode: 145, Loss: 0.1448986977338791\n",
      "Episode: 146, Loss: 0.6422885060310364\n",
      "Episode: 147, Loss: 0.4748261868953705\n",
      "Episode: 148, Loss: 0.07535460591316223\n",
      "Episode: 149, Loss: 0.10003656893968582\n",
      "Episode: 150, Loss: 0.15795695781707764\n",
      "Episode: 151, Loss: 0.44402527809143066\n",
      "Episode: 152, Loss: 0.07890116423368454\n",
      "Episode: 153, Loss: 0.6050782203674316\n",
      "Episode: 154, Loss: 0.10506350547075272\n",
      "Episode: 155, Loss: 0.11257418245077133\n",
      "Episode: 156, Loss: 0.6645305752754211\n",
      "Episode: 157, Loss: 0.14653867483139038\n",
      "Episode: 158, Loss: 0.6912131309509277\n",
      "Episode: 159, Loss: 0.190498486161232\n",
      "Episode: 160, Loss: 0.673330545425415\n",
      "Episode: 161, Loss: 0.736187756061554\n",
      "Episode: 162, Loss: 0.10256155580282211\n",
      "Episode: 163, Loss: 0.3088589906692505\n",
      "Episode: 164, Loss: 0.2702353000640869\n",
      "Episode: 165, Loss: 0.11866172403097153\n",
      "Episode: 166, Loss: 0.04793816804885864\n",
      "Episode: 167, Loss: 0.11258181929588318\n",
      "Episode: 168, Loss: 0.23490417003631592\n",
      "Episode: 169, Loss: 0.19960540533065796\n",
      "Episode: 170, Loss: 0.4240671694278717\n",
      "Episode: 171, Loss: 0.0578199066221714\n",
      "Episode: 172, Loss: 0.4732123613357544\n",
      "Episode: 173, Loss: 0.3571643829345703\n",
      "Episode: 174, Loss: 0.2891179621219635\n",
      "Episode: 175, Loss: 0.5660917162895203\n",
      "Episode: 176, Loss: 0.445713609457016\n",
      "Episode: 177, Loss: 0.6308090090751648\n",
      "Episode: 178, Loss: 0.042900051921606064\n",
      "Episode: 179, Loss: 0.13477188348770142\n",
      "Episode: 180, Loss: 0.4148149788379669\n",
      "Episode: 181, Loss: 0.21211719512939453\n",
      "Episode: 182, Loss: 0.2942785620689392\n",
      "Episode: 183, Loss: 0.623261034488678\n",
      "Episode: 184, Loss: 0.219319686293602\n",
      "Episode: 185, Loss: 0.5667359232902527\n",
      "Episode: 186, Loss: 0.11800874024629593\n",
      "Episode: 187, Loss: 0.07631036639213562\n",
      "Episode: 188, Loss: 0.46782946586608887\n",
      "Episode: 189, Loss: 0.5800583362579346\n",
      "Episode: 190, Loss: 0.15792106091976166\n",
      "Episode: 191, Loss: 0.12720602750778198\n",
      "Episode: 192, Loss: 0.0923222228884697\n",
      "Episode: 193, Loss: 0.21624590456485748\n",
      "Episode: 194, Loss: 0.28455618023872375\n",
      "Episode: 195, Loss: 0.2978653013706207\n",
      "Episode: 196, Loss: 0.10627948492765427\n",
      "Episode: 197, Loss: 0.10466039180755615\n",
      "Episode: 198, Loss: 0.21754837036132812\n",
      "Episode: 199, Loss: 0.35832807421684265\n",
      "Episode: 200, Loss: 0.6551459431648254\n",
      "Episode: 201, Loss: 0.7638487219810486\n",
      "Episode: 202, Loss: 0.15067888796329498\n",
      "Episode: 203, Loss: 0.47000131011009216\n",
      "Episode: 204, Loss: 0.2172068953514099\n",
      "Episode: 205, Loss: 0.3376547396183014\n",
      "Episode: 206, Loss: 0.44433069229125977\n",
      "Episode: 207, Loss: 0.7823583483695984\n",
      "Episode: 208, Loss: 0.5674448013305664\n",
      "Episode: 209, Loss: 0.7366082072257996\n",
      "Episode: 210, Loss: 0.8472220301628113\n",
      "Episode: 211, Loss: 0.08839923143386841\n",
      "Episode: 212, Loss: 0.08933485299348831\n",
      "Episode: 213, Loss: 0.6465124487876892\n",
      "Episode: 214, Loss: 0.19760499894618988\n",
      "Episode: 215, Loss: 0.43001291155815125\n",
      "Episode: 216, Loss: 0.1132306456565857\n",
      "Episode: 217, Loss: 0.12730379402637482\n",
      "Episode: 218, Loss: 0.5015429854393005\n",
      "Episode: 219, Loss: 0.7801818251609802\n",
      "Episode: 220, Loss: 0.19170404970645905\n",
      "Episode: 221, Loss: 0.5476625561714172\n",
      "Episode: 222, Loss: 0.6534217000007629\n",
      "Episode: 223, Loss: 0.22722160816192627\n",
      "Episode: 224, Loss: 0.16108138859272003\n",
      "Episode: 225, Loss: 0.12567546963691711\n",
      "Episode: 226, Loss: 0.3678872287273407\n",
      "Episode: 227, Loss: 0.29604923725128174\n",
      "Episode: 228, Loss: 0.5648525357246399\n",
      "Episode: 229, Loss: 0.22151905298233032\n",
      "Episode: 230, Loss: 0.7610592842102051\n",
      "Episode: 231, Loss: 0.8108331561088562\n",
      "Episode: 232, Loss: 0.4630730152130127\n",
      "Episode: 233, Loss: 0.4306265413761139\n",
      "Episode: 234, Loss: 0.22053773701190948\n",
      "Episode: 235, Loss: 0.33766409754753113\n",
      "Episode: 236, Loss: 0.7797643542289734\n",
      "Episode: 237, Loss: 0.12853297591209412\n",
      "Episode: 238, Loss: 0.08171050250530243\n",
      "Episode: 239, Loss: 0.27144932746887207\n",
      "Episode: 240, Loss: 0.5162320137023926\n",
      "Episode: 241, Loss: 0.2510022819042206\n",
      "Episode: 242, Loss: 0.18316985666751862\n",
      "Episode: 243, Loss: 0.24057839810848236\n",
      "Episode: 244, Loss: 0.07382645457983017\n",
      "Episode: 245, Loss: 0.1784994751214981\n",
      "Episode: 246, Loss: 0.1256909817457199\n",
      "Episode: 247, Loss: 0.2312939316034317\n",
      "Episode: 248, Loss: 0.31330379843711853\n",
      "Episode: 249, Loss: 0.4970135986804962\n",
      "Episode: 250, Loss: 0.36396729946136475\n",
      "Episode: 251, Loss: 0.09318064898252487\n",
      "Episode: 252, Loss: 0.745769739151001\n",
      "Episode: 253, Loss: 0.09715702384710312\n",
      "Episode: 254, Loss: 0.20684003829956055\n",
      "Episode: 255, Loss: 0.38714396953582764\n",
      "Episode: 256, Loss: 0.12733407318592072\n",
      "Episode: 257, Loss: 0.21327286958694458\n",
      "Episode: 258, Loss: 0.31977349519729614\n",
      "Episode: 259, Loss: 0.35291752219200134\n",
      "Episode: 260, Loss: 0.6472797393798828\n",
      "Episode: 261, Loss: 0.13895030319690704\n",
      "Episode: 262, Loss: 0.3434905707836151\n",
      "Episode: 263, Loss: 0.19264228641986847\n",
      "Episode: 264, Loss: 0.3412419259548187\n",
      "Episode: 265, Loss: 0.12380167841911316\n",
      "Episode: 266, Loss: 0.27672436833381653\n",
      "Episode: 267, Loss: 0.4680335819721222\n",
      "Episode: 268, Loss: 0.2687373459339142\n",
      "Episode: 269, Loss: 0.33280813694000244\n",
      "Episode: 270, Loss: 0.09537925571203232\n",
      "Episode: 271, Loss: 0.25789204239845276\n",
      "Episode: 272, Loss: 0.1821904182434082\n",
      "Episode: 273, Loss: 0.5273799300193787\n",
      "Episode: 274, Loss: 0.31774330139160156\n",
      "Episode: 275, Loss: 0.2239595204591751\n",
      "Episode: 276, Loss: 0.132384791970253\n",
      "Episode: 277, Loss: 0.09530016034841537\n",
      "Episode: 278, Loss: 0.28610536456108093\n",
      "Episode: 279, Loss: 0.08770743757486343\n",
      "Episode: 280, Loss: 0.10718387365341187\n",
      "Episode: 281, Loss: 0.12382462620735168\n",
      "Episode: 282, Loss: 0.38652685284614563\n",
      "Episode: 283, Loss: 0.43831801414489746\n",
      "Episode: 284, Loss: 0.2606070935726166\n",
      "Episode: 285, Loss: 0.11839741468429565\n",
      "Episode: 286, Loss: 0.3716825544834137\n",
      "Episode: 287, Loss: 0.29289448261260986\n",
      "Episode: 288, Loss: 0.2514616847038269\n",
      "Episode: 289, Loss: 0.23796093463897705\n",
      "Episode: 290, Loss: 0.30459949374198914\n",
      "Episode: 291, Loss: 0.23550374805927277\n",
      "Episode: 292, Loss: 0.18848465383052826\n",
      "Episode: 293, Loss: 0.11753088980913162\n",
      "Episode: 294, Loss: 0.1426735669374466\n",
      "Episode: 295, Loss: 0.2237706184387207\n",
      "Episode: 296, Loss: 0.34064868092536926\n",
      "Episode: 297, Loss: 0.3956052362918854\n",
      "Episode: 298, Loss: 0.2759193480014801\n",
      "Episode: 299, Loss: 0.3860265910625458\n",
      "Episode: 300, Loss: 0.33345791697502136\n",
      "Episode: 301, Loss: 0.3304559886455536\n",
      "Episode: 302, Loss: 0.34684404730796814\n",
      "Episode: 303, Loss: 0.20909661054611206\n",
      "Episode: 304, Loss: 0.24153365194797516\n",
      "Episode: 305, Loss: 0.31397998332977295\n",
      "Episode: 306, Loss: 0.060986559838056564\n",
      "Episode: 307, Loss: 0.19218118488788605\n",
      "Episode: 308, Loss: 0.1757715344429016\n",
      "Episode: 309, Loss: 0.19424991309642792\n",
      "Episode: 310, Loss: 0.20608359575271606\n",
      "Episode: 311, Loss: 0.1308613270521164\n",
      "Episode: 312, Loss: 0.38091644644737244\n",
      "Episode: 313, Loss: 0.4276483952999115\n",
      "Episode: 314, Loss: 0.5480674505233765\n",
      "Episode: 315, Loss: 0.3499065339565277\n",
      "Episode: 316, Loss: 0.3935508728027344\n",
      "Episode: 317, Loss: 0.22839121520519257\n",
      "Episode: 318, Loss: 0.2378804087638855\n",
      "Episode: 319, Loss: 0.08867043256759644\n",
      "Episode: 320, Loss: 0.3484428822994232\n",
      "Episode: 321, Loss: 0.25848427414894104\n",
      "Episode: 322, Loss: 0.2840597629547119\n",
      "Episode: 323, Loss: 0.34989532828330994\n",
      "Episode: 324, Loss: 0.4474889934062958\n",
      "Episode: 325, Loss: 0.6047481894493103\n",
      "Episode: 326, Loss: 0.14818492531776428\n",
      "Episode: 327, Loss: 0.17244887351989746\n",
      "Episode: 328, Loss: 0.45152416825294495\n",
      "Episode: 329, Loss: 0.14003460109233856\n",
      "Episode: 330, Loss: 0.34385696053504944\n",
      "Episode: 331, Loss: 0.1531120091676712\n",
      "Episode: 332, Loss: 0.13395695388317108\n",
      "Episode: 333, Loss: 0.10841155052185059\n",
      "Episode: 334, Loss: 0.03763555362820625\n",
      "Episode: 335, Loss: 0.4169778525829315\n",
      "Episode: 336, Loss: 0.25048330426216125\n",
      "Episode: 337, Loss: 0.06580626219511032\n",
      "Episode: 338, Loss: 0.5901281237602234\n",
      "Episode: 339, Loss: 0.13473261892795563\n",
      "Episode: 340, Loss: 0.3967674672603607\n",
      "Episode: 341, Loss: 0.2706310749053955\n",
      "Episode: 342, Loss: 0.42095503211021423\n",
      "Episode: 343, Loss: 0.1686779409646988\n",
      "Episode: 344, Loss: 0.12396419048309326\n",
      "Episode: 345, Loss: 0.11585727334022522\n",
      "Episode: 346, Loss: 0.23973625898361206\n",
      "Episode: 347, Loss: 0.5471823811531067\n",
      "Episode: 348, Loss: 0.28385013341903687\n",
      "Episode: 349, Loss: 0.38564562797546387\n",
      "Episode: 350, Loss: 0.24035727977752686\n",
      "Episode: 351, Loss: 0.21482159197330475\n",
      "Episode: 352, Loss: 0.2774094045162201\n",
      "Episode: 353, Loss: 0.37245723605155945\n",
      "Episode: 354, Loss: 0.8492843508720398\n",
      "Episode: 355, Loss: 0.14483779668807983\n",
      "Episode: 356, Loss: 0.07156043499708176\n",
      "Episode: 357, Loss: 0.44616571068763733\n",
      "Episode: 358, Loss: 0.6050503849983215\n",
      "Episode: 359, Loss: 0.15880966186523438\n",
      "Episode: 360, Loss: 0.18851123750209808\n",
      "Episode: 361, Loss: 0.11245567351579666\n",
      "Episode: 362, Loss: 0.9269072413444519\n",
      "Episode: 363, Loss: 0.1243126392364502\n",
      "Episode: 364, Loss: 1.1439841985702515\n",
      "Episode: 365, Loss: 0.12613274157047272\n",
      "Episode: 366, Loss: 1.3300212621688843\n",
      "Episode: 367, Loss: 0.2929829955101013\n",
      "Episode: 368, Loss: 0.24729514122009277\n",
      "Episode: 369, Loss: 0.37485507130622864\n",
      "Episode: 370, Loss: 0.1354294866323471\n",
      "Episode: 371, Loss: 0.6740312576293945\n",
      "Episode: 372, Loss: 0.26388978958129883\n",
      "Episode: 373, Loss: 1.3833526372909546\n",
      "Episode: 374, Loss: 0.07654281705617905\n",
      "Episode: 375, Loss: 1.310839295387268\n",
      "Episode: 376, Loss: 0.631199300289154\n",
      "Episode: 377, Loss: 0.5950556397438049\n",
      "Episode: 378, Loss: 0.7141851782798767\n",
      "Episode: 379, Loss: 0.3360207974910736\n",
      "Episode: 380, Loss: 0.7270687222480774\n",
      "Episode: 381, Loss: 0.5255494117736816\n",
      "Episode: 382, Loss: 0.25682058930397034\n",
      "Episode: 383, Loss: 0.7961885929107666\n",
      "Episode: 384, Loss: 0.5308642983436584\n",
      "Episode: 385, Loss: 0.9629891514778137\n",
      "Episode: 386, Loss: 1.6170930862426758\n",
      "Episode: 387, Loss: 1.6472686529159546\n",
      "Episode: 388, Loss: 0.11578001827001572\n",
      "Episode: 389, Loss: 1.6380691528320312\n",
      "Episode: 390, Loss: 0.3221895396709442\n",
      "Episode: 391, Loss: 1.603181004524231\n",
      "Episode: 392, Loss: 0.13771206140518188\n",
      "Episode: 393, Loss: 1.7941175699234009\n",
      "Episode: 394, Loss: 1.7967348098754883\n",
      "Episode: 395, Loss: 1.468990683555603\n",
      "Episode: 396, Loss: 0.1294887214899063\n",
      "Episode: 397, Loss: 1.5552153587341309\n",
      "Episode: 398, Loss: 1.7283964157104492\n",
      "Episode: 399, Loss: 1.3620686531066895\n",
      "Episode: 400, Loss: 0.8335168957710266\n",
      "Episode: 401, Loss: 1.7381114959716797\n",
      "Episode: 402, Loss: 0.12033689022064209\n",
      "Episode: 403, Loss: 1.6568942070007324\n",
      "Episode: 404, Loss: 1.7116189002990723\n",
      "Episode: 405, Loss: 0.12307638674974442\n",
      "Episode: 406, Loss: 0.09705085307359695\n",
      "Episode: 407, Loss: 0.6348535418510437\n",
      "Episode: 408, Loss: 0.15254853665828705\n",
      "Episode: 409, Loss: 0.4389707148075104\n",
      "Episode: 410, Loss: 0.17266006767749786\n",
      "Episode: 411, Loss: 0.5122812390327454\n",
      "Episode: 412, Loss: 0.2827146351337433\n",
      "Episode: 413, Loss: 0.3931431472301483\n",
      "Episode: 414, Loss: 0.5106812715530396\n",
      "Episode: 415, Loss: 0.17427676916122437\n",
      "Episode: 416, Loss: 0.1979457587003708\n",
      "Episode: 417, Loss: 0.12962807714939117\n",
      "Episode: 418, Loss: 0.9693558216094971\n",
      "Episode: 419, Loss: 0.09640253335237503\n",
      "Episode: 420, Loss: 0.1264975666999817\n",
      "Episode: 421, Loss: 0.7457773089408875\n",
      "Episode: 422, Loss: 0.19824965298175812\n",
      "Episode: 423, Loss: 0.10253789275884628\n",
      "Episode: 424, Loss: 0.12785392999649048\n",
      "Episode: 425, Loss: 0.13540227711200714\n",
      "Episode: 426, Loss: 0.3099653720855713\n",
      "Episode: 427, Loss: 0.29794177412986755\n",
      "Episode: 428, Loss: 0.2563778758049011\n",
      "Episode: 429, Loss: 0.17373336851596832\n",
      "Episode: 430, Loss: 0.5361964106559753\n",
      "Episode: 431, Loss: 0.21357941627502441\n",
      "Episode: 432, Loss: 0.21663261950016022\n",
      "Episode: 433, Loss: 0.20381473004817963\n",
      "Episode: 434, Loss: 0.28553274273872375\n",
      "Episode: 435, Loss: 0.15092553198337555\n",
      "Episode: 436, Loss: 0.4180103838443756\n",
      "Episode: 437, Loss: 0.0981803610920906\n",
      "Episode: 438, Loss: 0.23255562782287598\n",
      "Episode: 439, Loss: 0.2989027500152588\n",
      "Episode: 440, Loss: 0.137167826294899\n",
      "Episode: 441, Loss: 0.32909998297691345\n",
      "Episode: 442, Loss: 0.3447631895542145\n",
      "Episode: 443, Loss: 0.12411105632781982\n",
      "Episode: 444, Loss: 0.15447385609149933\n",
      "Episode: 445, Loss: 0.30656489729881287\n",
      "Episode: 446, Loss: 0.11822742968797684\n",
      "Episode: 447, Loss: 0.4064319133758545\n",
      "Episode: 448, Loss: 0.1532573252916336\n",
      "Episode: 449, Loss: 0.24557936191558838\n",
      "Episode: 450, Loss: 0.29195722937583923\n",
      "Episode: 451, Loss: 0.34763094782829285\n",
      "Episode: 452, Loss: 0.17582982778549194\n",
      "Episode: 453, Loss: 0.18841244280338287\n",
      "Episode: 454, Loss: 0.3654313087463379\n",
      "Episode: 455, Loss: 0.09172021597623825\n",
      "Episode: 456, Loss: 0.32986703515052795\n",
      "Episode: 457, Loss: 0.33740803599357605\n",
      "Episode: 458, Loss: 0.5367642045021057\n",
      "Episode: 459, Loss: 0.08767978101968765\n",
      "Episode: 460, Loss: 0.28778499364852905\n",
      "Episode: 461, Loss: 0.5358822345733643\n",
      "Episode: 462, Loss: 0.19463767111301422\n",
      "Episode: 463, Loss: 0.1926831156015396\n",
      "Episode: 464, Loss: 0.21158619225025177\n",
      "Episode: 465, Loss: 0.13445185124874115\n",
      "Episode: 466, Loss: 0.1562739759683609\n",
      "Episode: 467, Loss: 0.1984633356332779\n",
      "Episode: 468, Loss: 0.15886910259723663\n",
      "Episode: 469, Loss: 0.3766544759273529\n",
      "Episode: 470, Loss: 0.20496225357055664\n",
      "Episode: 471, Loss: 0.1551765352487564\n",
      "Episode: 472, Loss: 0.19371511042118073\n",
      "Episode: 473, Loss: 0.11127463728189468\n",
      "Episode: 474, Loss: 0.16713188588619232\n",
      "Episode: 475, Loss: 0.16778801381587982\n",
      "Episode: 476, Loss: 0.154354065656662\n",
      "Episode: 477, Loss: 0.10527190566062927\n",
      "Episode: 478, Loss: 0.13037435710430145\n",
      "Episode: 479, Loss: 0.45422014594078064\n",
      "Episode: 480, Loss: 0.15303516387939453\n",
      "Episode: 481, Loss: 0.1624307483434677\n",
      "Episode: 482, Loss: 0.6246637105941772\n",
      "Episode: 483, Loss: 0.16732437908649445\n",
      "Episode: 484, Loss: 0.1475726217031479\n",
      "Episode: 485, Loss: 0.37256181240081787\n",
      "Episode: 486, Loss: 0.5299069285392761\n",
      "Episode: 487, Loss: 0.1537628173828125\n",
      "Episode: 488, Loss: 0.11385353654623032\n",
      "Episode: 489, Loss: 0.16902215778827667\n",
      "Episode: 490, Loss: 0.17489682137966156\n",
      "Episode: 491, Loss: 0.14753873646259308\n",
      "Episode: 492, Loss: 0.16758771240711212\n",
      "Episode: 493, Loss: 0.24398499727249146\n",
      "Episode: 494, Loss: 0.1641007512807846\n",
      "Episode: 495, Loss: 0.11850637197494507\n",
      "Episode: 496, Loss: 0.1681807041168213\n",
      "Episode: 497, Loss: 0.11412960290908813\n",
      "Episode: 498, Loss: 0.16700617969036102\n",
      "Episode: 499, Loss: 0.1281348615884781\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
